\documentclass[pdftex,english,11pt,parskip=half]{scrartcl}
\usepackage{palatino}
\usepackage{mathpazo}
\usepackage[margin=0.7in]{geometry}
%\usepackage{parskip}
\usepackage[compact]{titlesec}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{babel}
\usepackage{framed}
\usepackage{wrapfig}
\usepackage{subfig}
\usepackage{sidecap}
\usepackage[labelfont=bf,font=small,format=plain]{caption}
\usepackage{doi}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage{array}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{float}
%\usepackage[authoryear]{natbib}
\usepackage[numbers]{natbib}
\usepackage{url,hyperref,color}
\usepackage{rotating}
\usepackage{lscape}
\definecolor{darkblue}{rgb}{0.0,0.0,0.75}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\newcommand{\fixme}[1]{{\color{red} #1}}
\renewcommand\thesection{\Alph{section}}
\renewcommand{\familydefault}{\sfdefault}
\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\begin{document}
\addtokomafont{section}{\large}
\def\bf{\normalfont\bfseries}
\pagestyle{empty}

\section*{Research Education Program Plan}

\section{Significance}
\vspace{-0.1in}

Current focal points of NIH-funded biomedical research include translational
medicine, big data analyses of genomic, proteomic, and metabolomic experiments,
and team science \cite{}, all of which rely on the generation, sharing, and
integration of high-quality experimental data that may be used and re-used in
multiple types of analysis and visualization platforms. The NIH Strategic Plan
for Data Science \cite{} emphasizes the critical need to capitalize on this
emerging ecosystem of data, and includes the need to address multiple practical
issues related to data storage, management, accessibility, and usablility.
However, consideration of the current sources of much of this data from
individual bench scientists and academic research groups, leads to the 
recognition that a large amount of biomedical data collection is in the form 
of commercial or open source spreadsheets that contain sometimes complex 
organizational
patterns of quantitative and qualitative tables, charts, graphs, notes and other
figures, embedded black-box statistical calculations, and mixtures of
experimental data types and units. Further, it is widely known and discussed
among data scientists, mathematical modelers, and statisticians \cite{}, that
there is a frequent need disgard, transform, and reformat sometimes large
amounts of this primary data and meta-data. While such ``data cleaning'' is
necessary for it's use in the advanced and developing data tools, it is often
conducted by the data-scientist with minimal or no communcation with those who
generated the data. From our experience with drug and vaccine development for
infectious diseases, \textbf{we have identified this step of data recording and
pre-processing as a potential critical point of failure for reproducible
research results.} In this proposal, we provide a solution to mitigate such
potential failures by using the R programming language and the concept of "tidy"
data stuructures, together with a set of training modules that are aimed at the
laboratory bench scientists whose attention is rather to their experimental
technique and collection of accurate data, and who may have little or no
background in the use of general purpose software tools.

While this proposal uses examples from microbiology and immunology experiments,
the collection of primary data in a flexible, open source, transpararent, 
and reproducible format, which can be kept in it's primary state without the 
need for additional modification, is a solution that extends thoughout
large and small scale biomedical science projects.  Figure () below shows an 
example workflow of such a comprehensive experimental data and quantitative 
analysis framework, and indicates our specific focus, highlighting it's 
critical place in such an analysis framework.

\section{Innovation} \vspace{-0.1in}

To disseminate the training materials we develop, \textbf{we will use the
innovative \textit{bookdown} framework \cite{xie2016bookdown} to structure and
publish all of our training materials as a free and open online book}. The
\textit{bookdown} framework has been available for a little over two years and
extends the principles of \textit{rmarkdown} to allow users to create attractive
online books that integrate programming code and text. This format allows
authors to efficiency create books with many coding examples---code and its
output does not need to be copied and pasted into a document, but instead can be
automatically generated each time the book is edited. Also, since the book is
created by executing those code examples, regularly test that all code remains
free of bugs created by changes in dependencies (e.g., updates to R packages
used in the code) \cite{xie2016bookdown}. Through this innovative framework, we
will be able to create a searchable online book that weaves R code into the text
and that can include embedded tutorial videos, active weblinks to online
references, and computationally reproducible practice examples and exercises
(Figure \ref{fig:prototype}). While relatively new, this framework has proven
itself reliable and effective---it serves as the framework for several extremely
popular and highly-accessed books on R programming, including \textit{R for Data
Science} [ref]. 

\begin{figure}[t] \includegraphics[width =
\textwidth]{figures/book_prototype.jpg} \caption{Prototype of online course
book, with features highlighted.} \label{fig:prototype} \end{figure}

Dr. Anderson (PI) was an early adopter of the \textit{bookdown} framework. Since
it became available, she has used it to create two \textit{bookdown}-based
books: the \textit{R Programming for Research} online coursebook, which she uses
as a joint textbook and website for the \textit{R Programming for Research}
course she teaches each fall at Colorado State University, and \textit{Mastering
Software Development in R}, which she co-wrote with Dr. Roger Peng as a manual
for developing advanced R programming skills and which has been downloaded by
over 14,000 people from LeanPub (see letter from Dr. Peng). Dr. Anderson served as one of the six
reviewers---along with R programming experts Jan de Leeuw, Karl Broman, Michael
Grayling, Daniel Kaplan, and Max Kuhn---for \textit{bookdown: Authoring Books
and Technical Documents with R Markdown} \cite{xie2016bookdown}.

\textbf{The use of this framework will help us effectively reach a large
audience in need of training materials for improving the reproducibility of data
recording and pre-processing in biomedical research.} We will be able to freely
post this book online using the Git Pages feature of GitHub, as we have done
with previous books created with the same framework. This will allow anyone to
freely access this content online, and to explore the module contents to find
the set of modules that best suits their immediate training needs. The
book-style format makes the content easy to navigate through both its use of
sections and chapters to group material and through its embedded search
functionality (Figure \ref{fig:prototype}). Further, researchers will be able to
download the book as either a PDF or EPUB file, to use as a reference as they
continue learning to implement reproducibility tools in their research projects.
[More about this.]

We will publish the book's code openly online through GitHub from the beginning
of our development of the training materials, as well as publish the current
version of materials as the online book. \textbf{By making the training
materials available from day one, as they are developed, we will be able to
attract early users to help disseminate the material as it evolves and also be
able to get early feedback on the content as it is developed.} Once the online
book is developed, we will be able to submit a static version of it to be posted
on the homepage of the \textit{bookdown.org} website [ref], which will serve as
another way for trainees to find and access the materials.

\section{Approach---Proposed Research Education Program}

\textbf{Project goals and objectives.} Our project's primary goal is to develop
training modules that address the needs of laboratory-based biomedical
researchers seeking to improve reproducibility, especially of experimental data
recording and pre-processing, in their research projects. The expected result of
this project is an online book that contains twenty short training modules as separate
chapters, with video lectures, written text, and additional educational
materials collected within each module's chapter. We consider it critical that
these training materials be clear, relevant, and useful to a key audience of
biomedical scientists whose primary research activities focus on laboratory
research, rather than data analysis or statistics. To help us achieve this goal,
our team includes three co-investigators from this key audience who will play an
early and continuing role in developing and refining the training materials for
this audience, and our plan includes intensive user testing of the developing
materials on several groups of pilot testers from this audience. Our
co-investigators' network of connections in Microbiology and Immunology will
help us widely disseminate these materials to our target audience, as will the
free, online format of the materials and presenting the materials at national
and international Microbiology and Immunology conferences in the second and
third years of the project.

\begin{itemize}
\item ``Without formal requirements for continuing education, the most effective solutions may be to develop educational resources that are accessible, easy-to-digest and immediately and effectively applicable to research (for example, brief, web-based modules for specific top- ics, and combinations of modules that are customized for particular research applications). A modular approach simplifies the process of iterative updating of those materials. Demonstration software and hands-on examples may also make the lessons and implications par- ticularly tangible to researchers at any career stage" \cite{munafo2017manifesto}\item ``Studies of statistical power persistently find it to be below (sometimes well below) 50\%, across both time and the different disciplines studied [2,35,36]. Low statistical power increases the likelihood of obtaining both false-positive and false-negative results [2], meaning that it offers no advantage if the purpose is to accumulate knowledge. Despite this, low-powered research persists because of dysfunctional incentives, poor understanding of the consequences of low power, and lack of resources to improve power. Team science is a solution to the latter problem---instead of relying on the limited resources of single investigators, distributed collaboration across many study sites facilitates high-powered designs and greater potential for testing generalizability across the settings and populations sampled. This also brings greater scope for multiple theoretical and disciplinary perspectives, and a diverse range of research cultures and experiences, to be incorporated into a research project." \cite{munafo2017manifesto}
\item ``Multi-centre and collaborative efforts have a long and successful tradition in fields such as randomized controlled trials in some areas of clinical medicine, and in genetic association analyses, and have improved the robustness of the resulting research literatures. Multi-site collaborative projects have also been advocated for other types of research, such as animal studies [41--43], in an effort to maximize their power, enhance standardization, and optimize transparency and protection from biases. The Many Labs projects illustrate this potential in the social and behavioural sciences, with dozens of laboratories implementing the same research protocol to obtain highly precise estimates of effect sizes, and evaluate variability across samples and settings [44,45]. It is also possible, and desirable, to incorporate a team science ethos into student training " \cite{munafo2017manifesto}
\item ``According to a U.S. National Science Foundation (NSF) subcommittee on replicability in science (9), 'reproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original inves- tigator. That is, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis in an attempt to yield the same results.... Reproducibility is a minimum necessary condition for a finding to be believable and informative.' ... Reproducibility defined in this way mainly addresses issues of trust that data and analyses are as represented. The definition does not specify to what extent deviations are acceptable. Such reproducibility does not add new evidential weight, although greater subjective weight is often accorded to evidence that is more highly trusted. New evidence is provided by new experimentation, defined in the NSF report as 'replicability,' which refers to 'the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected.'" \cite{goodman2016does}
\item ``How can we dramatically scale up data science education in the short term? One approach that we have taken is through massive online open courses (MOOCs). The Johns Hopkins Data Science Speciali- zation (jhudatascience.org) is a sequence of nine courses covering the full spectrum of data science skills from formulating quan- titative questions, to cleaning data, to sta- tistical analysis and producing reproducible reports. Thus far, we have enrolled more than 1.5 million students in this Specializa- tion. A complementary approach is crowd- sourced short courses such as Data and Soft- ware Carpentry (software-carpentry.org) that have addressed the extreme demand for data science knowledge on a smaller scale. However, simply increasing data analytic literacy comes at a cost. Most scientists in these programs will receive basic to moderate training in data analysis, creating the potential for producing individuals with enough skill to perform data analysis but without enough knowledge to pre- vent mistakes.
To improve the global robustness of scientific data analysis, we must couple education efforts with the identification of data analytic strategies that are most re- producible and replicable in the hands of basic or intermediate data analysts. Statisti- cians must bring to bear their history of developing rigorous methods to the area of data science." \cite{leek2015opinion}
\item ``Novices program differently than experts [26] and need different approaches or tools. If you ask a professional programmer to iterate over a list of integers and produce the average, they can write the code within seconds, using stored knowledge of the exact pattern required. Novices will approach this problem totally differently: they need to remember the syntax for the different parts, know how to iter- ate over a list, know how to use an accumulator variable, and so on.
Novices may need to spend time thinking about an algorithm on paper (something expert programmers rarely need, as they have usually memorised most common algorithmic pat- terns). They may need to construct examples in guided steps. They may struggle to debug. Debugging usually involves contrasting what is happening to what should be happening, but a novice’s grasp on what should be happening is usually fragile.
Novices do not become professionals simply by doing what professionals do at a slower pace. We do not teach reading by taking a classic novel and simply proceeding more slowly. We teach by using shorter books with simpler words and larger print. So in programming, we must take care to use small, self-contained tasks at a level suitable for novices, with tools that suit their needs and without scoffing." \cite{brown2018ten}
\item ``Our final tip for teaching programming is that you don’t have to program to do it. Faced with the challenges of learning syntax, semantics, algorithms, and design, examples that seem small to instructors can still easily overwhelm novices. Breaking the problem down into smaller sin- gle-concept pieces can reduce the cognitive load to something manageable." \cite{brown2018ten}
\end{itemize}

\subsubsection*{Proposed training modules}

\begin{quotation} ``State the \textbf{goals for education} and \textbf{justify
the area of training} selected for module development in terms of its
\textbf{relevance and potential impact} on improving the development of skills
and knowledge important for conducting rigorous and reproducible research.
Describe the \textbf{subject material} to be covered. The \textbf{length} of the
proposed training module should be explained in terms of \textbf{scope and depth
of coverage} of the subject matter.  In addition, \textbf{how the research
education will be utilized by trainees or investigators} should be
described---for example, a module on how to avoid confirmation bias to be taken
by all beginning laboratory workers, or a module on appropriate design of animal
studies to be taken immediately prior to beginning such work." \end{quotation}

Our emphasis on primary experimental data collected and recorded as a data
structure, specifically the innovative "tidy" data format developed within the R
statistical programming environment, and presented in an innovative e-book
format

[A paragraph on why we, specifically, passionately think that these training
modules would address a key need.] Our team combines experts in R programming
(Anderson, Lyons) with a group of biomedical researchers (Gonzalez-Juarrero,
Henao-Tamayo, and Robertson) who have, collectively, spent decades in
laboratory-based research to improve understanding of tuberculosis and other
diseases. We met as faculty members of the same College at Colorado State
University, and since have discovered how many of the tools that Drs. Anderson
and Lyons teach and use to improve the reproducibility of \textit{data analysis}
for biomedical research could substantially improve reproducibility and
trasparency in the laboratory-based biomedical research projects of Drs.
Gonzalez-Juarrero, Henao-Tamayo, and Robertson at the stages of \textit{data
recording} and \textit{data pre-processing}. Over the past year, we have begun
to work together to do this within our own research projects: for example, in
Fall 2017 Dr. Gonzalez-Juarrero attended Dr. Anderson's (PI) course in \textit{R
Programming for Research} and has brought the ideas and techniques back to her
research laboratory, in Fall 2017 Dr. Lyons worked with Dr. Anderson to bring in
real tuberculosis drug development data to use in the final group project in Dr.
Anderson's R Programming course, and in Spring 2018 Dr. Henao-Tamayo and Dr.
Anderson began co-advising a graduate student with the aims of implementing
open-source tools for pre-processing flow cytometry in Dr. Henao-Tamayo's
laboratry. Collectively, we are passionate about the idea that
\textbf{open-source tools can be used to bring substantial improvements to
reproducibility of data recording and pre-processing in laboratory-based
research}, and yet we are also able to recognize the key barriers in
implementing these tools in this setting, as well as in training
laboratory-based researchers in how to use these tools, and why existing free
training materials have, to date, been limited in meeting these needs for the
key audience of laboratory-based biomedical researchers. 

[Paragraph on why we are focusing on data recording and data pre-processing.]
Many excellent free training resources exist to improve the computational
reproducibility of biomedical research. However, most of these
materials---including some developed by Dr. Anderson for her open online and
CSU-based courses in R programming [refs]---target researchers at the stage of
\textit{data analysis}, and provide much less guidance on the principles and
techniques to improve reproducibility of the earlier steps of
\textbf{experimental data recording} and \textbf{experimental data
pre-processing}. In this project, we will create training modules to fill this
gap. [More on this, ideally with some references to the importance of
reproduciblity in these steps.]

[A paragraph on who our key audience is and why we are focusing on that
audience.] A key aim is to make these modules \textbf{accessible and useful to
our target audience, laboratory-based researchers}. Three of the Co-Is on our
team (Gonzalez-Juarrero, Henao-Tamayo, and Robertson) represent this key
audience. [Why it is so important to develop these training materials for this
audience.]

[A paragraph about how we plan to focus on this audience] We plan to make these
modules accessible and useful to our target audience, laboratory-based
researchers by including examples from real microbiology and immunology research
projects and by piloting the training modules among laboratory-based biomedical
researchers. Our training materials will provide resources that can be used by
this audience of researchers at a variety of levels, from undergraduate students
to principal investigators, with a modular design that will allow a trainee to
use the subset of modules the best meets his or her immediate needs.

[A paragraph on how our training materials will fit in with the training
materials developed under previous rounds of this grant.]

We aim to create training modules that will show laboratory-based biomedical
researchers how simple computational reproducibility principles can improve
biomedical research reproducibility at the stages of data recording and
preprocessing and teach them how to implement these principles in their
laboratories. The importance of computational reproducibility of scientific
research is increasingly recognized by scientists, journals, and funding
agencies, with such ``computationally reproducible" research requiring that all
data and code for a research project be available and that this data and code
can be used to regenerate study findings either by the original researcher or by
other researchers \cite{ellis2018share, ram2013git}. Among our team, we have
found that there are many common existing practices---including use of
spreadsheets with embedded macros to concurrently record and analyze
experimental data, unstructured [too strong?] management of project files,
reliance on proprietory, vendor-supplied point-and-click software for data
pre-processing---that can interfere with the transparency, reproducibility, and
efficiency of the kind of research they conduct. [Some more about how we're not
the first people to notice that this is a problem for reproducibility / rigor.] 

We propose to develop two collections of modules, \textbf{Improving the
Reproducibility of Experimental Data Recording} and \textbf{Improving the
Reproducibility of Experimental Data Pre-Processing}. Our team has worked
together to create a curriculum of training modules that we believe will help
fill an important training gap for laboratory-based biomedical researchers
(Tables \ref{tab:content_one} and \ref{tab:content_two}). The first sequence,
\textbf{``Improving the Reproducibility of Experimental Data Recording"; Table
\ref{tab:content_one}}, will explore the pitfalls of combining experimental data
recording and analysis within macro-enabled spreadsheets, explain the power
structured data formats for recording data, describe how reproducibility can be
improved by using a single structured directory to store all research project
files, and demonstrate the use of version control to maintain single, current
versions of all files while saving a history of all file changes. The second
sequence, \textbf{``Improving the Reproducibility of Experimental Data
Pre-Processing"; Table \ref{tab:content_two}}, will focus on improving the
reproducibility of experimental data pre-processing steps, like gating for flow
cytometry data and peak finding / quantifying for mass spectrometry data.
Training materials will explain how the use of code scripts for these steps
dramatically improves reproducibility compared to using vendor-supplied
point-and-click software and will introduce trainees to popular R software for
this pre-processing. This sequence will include advice on reproducible data
pre-processing protocols and how to create them using literate programming tools
(\textit{Rmarkdown}).

Each module will fall into one of three categories for teaching reproducibility:
(1) principals; (2) implementation; and (3) case study examples. ``Principals"
modules will be programming-language agnostic, while ``Implementation" modules
will focus on tools available through the popular open source R software and its
RStudio interface. Working with the biomedical laboratory-based co-investigators
on our team, we will ensure that these modules and the examples used in them are
approachable and useful to researchers with limited computational training. We
have divided the content into modules in a way that will allow \textbf{trainees
and investigators at any level} to create their own ``tracks" by selecting
relevant subsets of the modules to complete, and potentially combining this
content with other training modules available through the NIH's [Clearinghouse]
[ref]. Table \ref{tab:tracks} gives a few examples of how different trainees
could create and follow their own ``track" of the material.

For many of the barriers we identified to reproducibility at these stages of
research, a simple fix exists. Our team has worked together to craft a list of
specific topics (Tables \ref{tab:content_one} and \ref{tab:content_two}) we aim
to address with these modules so that they will offer training in a collection
of principles and techniques that tackle barriers to reprodubility with
straightforward fixes, but which we have found are still very common in
biomedical laboratory-based research programs, including those here at Colorado
State University. We have identified several easy-to-address barriers to
computational reproducibility at two stages of biomedical research: experimental
data recording and experimental data pre-processing.

\begin{itemize}
\item ``From our experience in the Leek group (where we work with a large number of collaborators to analyze data) and from conversations with other statisticians, the primary source of delay in the speed of returning results to collaborators is the con- dition of the data when they arrive." \cite{ellis2018share} `
\item `Consistent data sharing reduces the likelihood of errors during analysis and also decreases analysis turnaround time [when collaborating with statisticians."  \cite{ellis2018share} 
\item ``When a researcher turns over a properly tidied dataset, it dramatically decreases the workload on the statistician and minimizes the likelihood of errors during analysis."  \cite{ellis2018share}
\item ``Unquestionably, a significant contribu- tor to failure in oncology trials is the qual- ity of published preclinical data. Drug development relies heavily on the literature, especially with regards to new targets and biology. Moreover, clinical endpoints in can- cer are defined mainly in terms of patient survival, rather than by the intermediate endpoints seen in other disciplines (for example, cholesterol levels for statins). Thus, it takes many years before the clinical appli- cability of initial preclinical observations is known. The results of preclinical studies must therefore be very robust to withstand the rigours and challenges of clinical trials, stemming from the heterogeneity of both tumours and patients. The scientific community assumes that the claims in a preclinical study can be taken at face value — that although there might be some errors in detail, the main message of the paper can be relied on and the data will, for the most part, stand the test of time. Unfor- tunately, this is not always the case. Although the issue of irreproducible data has been discussed between scientists for decades, it has recently received greater attention (see go.nature.com/q7i2up) as the costs of drug development have increased along with the number of late-stage clinical-trial failures and the demand for more effective therapies." \cite{begley2012drug}
\item ``Although the importance of multiple studies corroborating a given result is acknowledged in virtually all of the sciences (Fig. 1), the modern use of “reproducible research” was originally applied not to corroboration, but to transparency, with application in the com- putational sciences. Computer scientist Jon Claerbout coined the term and associated it with a software platform and set of proce- dures that permit the reader of a paper to see the entire processing trail from the raw data and code to figures and tables (4)." \cite{goodman2016does}
\item ``Statistical methods can be complex, and continue to evolve in many specialties, particularly novel ones such as omics. However, statisticians and methodologists are only sporadically involved, often leading to flawed designs and analyses.72 Much flawed and irreproducible work has been published, even when only simple statistical tests are involved. Investigators of one study73 examined the use of Fisher’s exact test in 71 articles from six major medical journals. When a statistician was a member of the team, the test was used more appropriately than when one was not. Data that are multidimensional (ie, contain many features) are particularly at risk of false positives and overfitting, particularly when analysed by inexperienced or untrained analysts. Problems with statistical analysis might not be identified in peer review, especially when the report is not assessed by a statistician or methodologist." \cite{ioannidis2014increasing}
\item ``Little evidence exists about the research training of laboratory scientists. The way that many laboratory studies are reported suggests that scientists are unaware that their methodological approach is without rigour (figure). Many laboratory scientists have insufficient training in statistical methods and study design. This issue might be a more important deficiency than is poor training in clinical researchers, especially for laboratory investigation done by one scientist in an isolated laboratory—by contrast, many people would examine a clinical study protocol and report." \cite{ioannidis2014increasing}
\item ``Statisticians and methodologists should be involved in all stages of research. This recommendation has been repeatedly discussed, mostly for clinical trials, but it applies to all types of studies. Enhancement of communication between methodologists and other health scientists is also important." \cite{ioannidis2014increasing}
\item These are early steps of the ``dry lab" part of an experiment.
\item ``We define reproducibility as the ability to recompute data analytic results given an observed dataset and knowledge of the data analysis pipeline. The replicability of a study is the chance that an independent experi- ment targeting the same scientific question will produce a consistent result (1). ... There have been some very public failings of reproduc- ibility across a range of disciplines from can- cer genomics (3) to economics (4), and the data for many publications have not been made publicly available, raising doubts about the quality of data analyses." \cite{leek2015opinion}
\item ``From a computational perspective, there are three major components to a reproducible and replicable study: (i) the raw data from the experiment are available, (ii) the statisti- cal code and documentation to reproduce the analysis are available, and (iii) a correct data analysis must be performed." \cite{leek2015opinion}
\item ``If we can prevent prob- lematic data analyses from being conducted, we can substantially reduce the burden on the community of having to evaluate an increasingly heterogeneous and complex population of studies and research findings. The best way to prevent poor data analysis in the scientific literature is to (i) increase the number of trained data analysts in the scientific community and (ii) identify sta- tistical software and tools that can be shown to improve reproducibility and rep- licability of studies." \cite{leek2015opinion}
\item ``In today’s technology-driven era of biological discovery, many biologists generate extremely large datasets, including high-throughput sequencing, proteomic and metabolomic spectra, and high-content imaging screens. Subsequently, biologists must overcome many challenges of incorporating computing into their standard procedures for data analysis, including installing and running software that is not 'point-and-click,' navigating at the command-line interface, comparing various analysis tools that supposedly perform the same tasks, establishing effective note-taking for their computing trials, and managing large datasets. Especially for trainees, it can be overwhelming to know how to begin to address these challenges. Providing a roadmap for workflow approach and management is likely to accelerate biologists’ skill-building in computing." \cite{shade2015computing}
\item ``Skills in computing can enhance biologists’ logic and capacity for experimental design, increase understanding and interpretation of results, and promote interdisciplinary science by building a shared vocabulary and experience with collaborators in computer science, bioinformatics, statistics, physics, and engineering. We’ve suggested a systematic roadmap for computing workflows for biologists, including considering the overarching goals of the workflow, taking an iterative approach to analysis, implementing reproducibility checkpoints, recording effective computing notes, and adopting a team approach to analysis." \cite{shade2015computing}
\item In fMRI: ``Although we aim for reproduction of results with other data and independent analysis methods, the first step is to ensure that results can be replicated within la- boratories. This seems an easy task, but it is in fact com- mon that results cannot be replicated after, say, a year or two, when the student or post-doc responsible for the analyses and the data management has left. Increasing our capacity to replicate the data analysis workflow has another crucial aspect: this will allow us to better docu- ment our work, and therefore communicate and share it much more easily. It is crucial that we remember that resources are limited, and part of our work is to make it easy for others to check and build upon our findings." \cite{pernet2015improving}
\item In fMRI: ``In computer science and related communities, a number of informatics tools and software are available (databases, control version system, virtual machines, etc.) to handle data and code, check results and ensure reproducibility. Neuroscientists working with functional MRI are, how- ever, largely from other communities such as biology, medicine and psychology. Because of the differences in training and the field of research, such informatics tools are not necessarily sufficient, and are certainly not fully ac- cessible to or mastered by all researchers. In this review, we address specifically the community of neuroscientists with little programming experience, and point to a num- ber of tools and practices that can be used today by any- one willing to improve his or her research practices, with a view to better reproducibility." \cite{pernet2015improving}
\item ``We define reproducibility as the ability of an entire experi- ment to be reproduced [16], from data acquisition to re- sults." \cite{pernet2015improving}
\item For basic and preclinical research: ``Basic and preclinical research is particularly important
because it forms the foundation on which future studies are
built. It is preclinical research that provides the exciting, new
ideas that will eventually find their way into clinical studies
and new drugs that provide benefit to humankind. Yet this preclinical
research is poorly predictive of ultimate success in the
clinic.14,15 And it is observational research that both attracts
immediate public attention and often provides the hypothesis
on which interventional studies are based." \cite{begley2015reproducibility}
\item The costs to improving the computational reproducibility of a study are cheap compared to the costs of replicating a study.
\item ``What is difficult to quantify is the opportunity cost associated
with studies that fail to replicate. Investigators who pursue
a provocative and exciting idea but that is not based on robust
data will consume wasted hours for an idea that is ultimately
discarded. For ideas that proceed into late stages of clinical
evaluation or are even adopted in clinical practice only to be
discarded subsequently,17 the cost can be enormous." \cite{begley2015reproducibility}
\item ``It was recognized over a decade ago that experiments involving
array technology were potentially fraught with problems.
To address this, a requirement for mandatory full data
deposition was recommended in 2001 and quickly adopted by
many journals. This minimum information about a microarray
experiment standard has been widely accepted by investigators
and journals. However, despite being accepted as the desired
norm over a decade ago, compliance remains a problem.
An analysis of data from high-profile microarray publications
that appeared between 2005 and 2006 in Nature Genetics reproduced
the results of only 2 of 18 reanalyzed papers. The
principal reason for failure was lack of availability of original
raw data.22 A study of 127 articles on microarray studies published
between July 2011 and April 2012 revealed that ≈75%
were still not minimum information about a microarray experiment
compliant. Furthermore, reanalysis of data often did not
support the original conclusions.30" \cite{begley2015reproducibility}
\end{itemize}

\underline{\textbf{Improving the Reproducibility of Experimental Data
Recording}} 

The first sequence will provide principles and tools for increasing the
computational reproducibility at an early stage of biomedical research, as
experimental data is recorded. Key content will come from recent papers on improving computational reproducibility \cite{ellis2018share, broman2018data}. This sequence will include eleven modules covering
four main topics: 

\textbf{Separating data recording and analysis.} Many biomedical laboratories currently use spreadsheets, with embedded macros, to both record and analyze experimental data. This practice empedes the transparency and reproducibility of both data recording and data analysis. ... (Figure \ref{fig:spreadsheet}). A key source for this tutorial will be \cite{broman2018data}.

\begin{itemize}
\item ``If the scientist is sharing his or her data with the collaborator in Excel, the tidy data should be in one Excel file per table. The Excel file should not have multiple worksheets, no macros should be applied to the data, and no cells should be highlighted. Alterna- tively, the data could be shared in either a CSV or TAB-delimited text file. Caution should always be taken when reading CSV files into Excel as the use of Excel can sometimes lead to non-reproducible handling of date variables, time variables, and vari- ables that Excel incorrectly assumes are date or time variables (Zeeberg et al. 2004). For example, Excel incorrectly assumes the gene SEPT9 is the date Sept-9 due to default date format conversions. Floating-point format conversions cause sim- ilar problems. To avoid these issues, use ISO 8601 (Newman and Klyne n.d.) guidelines when coding date and time variables. (See Figure 2). Whenever data are formatted, the person tidying the data must be extremely careful that no unintended alterations have been made. Spot checking data after tidying, which includes, but is not limited to, ensuring the correct number of columns and rows are present and that column labels are accurate and consistent across spreadsheets, is crucial." \cite{ellis2018share}
\item Statisticians have outlined the methods that an experimental scientist can take to ensure that data shared in an Excel spreadsheet are shared in a reliable and reproducible way, including avoiding macros, using a separate Excel file for each dataset, recording descriptions of the variables in a separate code book rather than in the Excel file, avoiding the use of color of the cells to encode information, using ``\texttt{NA}" to code missing values, avoiding spaces in column headers, and avoiding spliting or merging cells \cite{ellis2018share, broman2018data}.
\item ``Spreadsheets, for all of their mundane rectangularness, have been the subject of angst and controversy for decades. ... Amid this debate, spreadsheets have continued to play a significant role in researchers’ workflows, and it is clear that they are a valuable tool that researchers are unlikely to abandon completely. The dangers of spreadsheets are real, however—so much so that the European Spreadsheet Risks Interest Group keeps a public archive of spreadsheet 'horror stories' (http://www.eusprig.org/horror-stories.htm). Many researchers have examined error rates in spreadsheets, and Panko (2008) reported that in 13 audits of real-world spreadsheets, an average of 88\% contained errors. Popular spreadsheet programs also make certain types of errors easy to commit and difficult to rectify. Microsoft Excel converts some gene names to dates and stores dates differently between operating systems, which can cause problems in downstream analyses (Zeeberg et al. 2004; Woo 2014). Researchers who use spreadsheets should be aware of these common errors and design spreadsheets that are tidy, consistent, and as resistant to mistakes as possible." \cite{broman2018data} 
\item ``Spreadsheets are often used as a multipurpose tool for data entry, storage, analysis, and visualization. Most spreadsheet programs allow users to perform all of these tasks, however we believe that spreadsheets are best suited to data entry and storage, and that analysis and visualization should happen sep- arately. Analyzing and visualizing data in a separate program, or at least in a separate copy of the data file, reduces the risk of contaminating or destroying the raw data in the spreadsheet." \cite{broman2018data} 
\item ``In this article, we offer practical recommendations for orga- nizing spreadsheet data in a way that both humans and computer programs can read. By following this advice, researchers will cre- ate spreadsheets that are less error-prone, easier for computers to process, and easier to share with collaborators and the pub- lic. Spreadsheets that adhere to our recommendations will work well with the tidy tools and reproducible methods described elsewhere in this collection and will form the basis of a robust and reproducible analytic workflow." \cite{broman2018data} 
\item ``Often, the Excel files that our collaborators send us include all kinds of calculations and graphs. We feel strongly that your pri- mary data file should contain just the data and nothing else: no calculations, no graphs.
If you are doing calculations in your data file, that likely means you are regularly opening it and typing into it. Doing so incurs some risk that you will accidentally type junk into your data. ... Your primary data file should be a pristine store of data. Write-protect it, back it up, and do not touch it." \cite{broman2018data} 
\item ``You might be tempted to highlight particular cells with suspicious data, or rows that should be ignored. Or the font or font color might have some meaning. Instead, add another column with an indicator variable. ... The highlighting is nice visually, but it is hard to extract that information for use in the later analysis. Analysis programs can much more readily handle data that are stored in a column than data encoded in cell highlighting, font, etc. (and in fact this markup will be lost completely in many programs). Another possible use of highlighting would be to indicate males and females in a mouse study by highlighting the corresponding rows in different colors. But rather than use highlight- ing to indicate sex, it is better to include a sex column, with values Male or Female." \cite{broman2018data}
\item ``A research compendium should maintain a clear separation of data, method, and output, while unambiguously expressing the relationship between those three. ... Keeping data and method separate treats the data as 'read-only,' so that the original data are untouched and all modifications are transparently documented in the code. ... In his advice to industry data scientists, Ben Baumer's article in this collection simi- larly highlights the importance of keeping data separate from the presentation of data, or research outputs." \cite{marwick2018packaging}
\end{itemize}

This topic will covered in a \textit{Principles} module on ``Separating data
recording and analysis".

\begin{figure}[b] \centering \includegraphics[width =
0.8\textwidth]{figures/algorithms.png} \caption{An \textit{xkcd} cartoon
captures the ballooning complexity and lack of transparency that can result from
many people using a spreadsheet with embedded macros. This practice can be a
critical barrier to reproducibility and transparency in biomedical research, yet
is still a common practice in many biomedical laboratories. \textit{Source: xkcd
by Randall Munroe}} \label{fig:spreadsheet} \end{figure}

\textbf{Using a structured data format to record data.} Every extra step of data
formatting is another chance to introduce an error in the data. The format in which experimental data is recorded can have a large influence on how easy and likely it is to implement reproducibility tools in later stages of data pre-processing, analysis, and visualization. Recording data in a ``structured" format brings many benefits for later stages of the research process, especially in terms of improving reproducibility.Therefore, by keeping research data pipelines simple---which can be more easily achieved if
data is initially recorded in a format amenable to later data pre-processing,
analysis, and visualization---researchers can decrease the potential for errors
in the data and therefore improve the rigor and reproducibility of their
research. 

\begin{itemize}
\item ``A set of general principles for sharing data have emerged within the statistics community (Broman and Woo 2017; Wickham 2014; Wilson et al. 2014; Wilson et al. 2017; White et al. 2013). But these principles are not always clear to researchers, scien- tists, or collaborators generating the data. This has led to a disconnect between those generating data and those analyzing it about the best way for data to be shared." \cite{ellis2018share} 
\item Following certain principals when sharing data for collaboration can ``avoid the most common pitfalls and sources of delay in the transition from data collection to data analysis (Leek and Peng 2015)". \cite{ellis2018share} 
\item Statisticians have advised that providing data to collaborators in a structured format (specifically the ``tidy" implementation) can improve the reproducibility of research, reduce the opportunity for errors being introduced when a collaborator cleans data from an unstructured format, and also increase the speed with which the collaborator can return their results \cite{ellis2018share}. 
\item A structured data format: ``Briefly, it is best to include a row at the top of each data table or spreadsheet that contains informative column names. And, each cell should include only one value or unit of information. Sentences should generally be avoided here; any lengthy explanations should instead be included in the 'code book.'" \cite{ellis2018share} 
\item ``The general principles of tidy data have been laid out previously (Wickham 2014). While the article describes tidy data using R (R: The R Project for Statistical Computing n.d.), the principles are more generally applicable:
1. Each variable should be in one column.
2. Each different observation of that variable should be in a
different row.
3. There should be one table for each 'kind' of data.
4. If there are multiple tables, each table should include a
column in the table (with the same column label!) that allows them to be joined or merged (see Figures 1(a) and (b))." \cite{ellis2018share}
\item ``When a researcher turns over a properly tidied dataset, it dramatically decreases the workload on the statistician and minimizes the likelihood of errors during analysis. By taking the time to tidy the data, the data generator, who knows the details of the data generated better than anyone else, can expect to get the analysis back sooner and can be more confident in its accuracy." \cite{ellis2018share}
\item It is very important to use the \textit{same} structured format across all of the files in the research project the record the same type of data. 
\item ``Use a consistent data layout in multiple files. If your data are in multiple files and you use different layouts in different files, it will be extra work for the analyst to combine the files into one dataset for analysis. With a consistent structure, it will be easy to automate this process." \cite{broman2018data}
\item ``Researchers who use spreadsheets should be aware of these common errors and design spreadsheets that are \textbf{tidy}, consistent, and as resistant to mistakes as possible." \cite{broman2018data} 
\item ``The best layout for your data within a spreadsheet is as a single big rectangle with rows corresponding to subjects and columns corresponding to variables. The first row should contain variable names, and please do not use more than one row for the variable names. ... The data files that we receive are usually not in rectangular form. More often, there seem to be bits of data sprinkled about." \cite{broman2018data} `
\item `If, from the start, the data were organized as a rectangle, it would save the analyst a great deal of time." \cite{broman2018data} ``Reorganizing the data into a 'tidy' format can simplify later analysis. But the rectangular aspect is the most important part." \cite{broman2018data}
\end{itemize}

The 'tidy' data format is one implementation of a structured data format that was introduced in a 2014 paper and has since quickly gained popularity among statisticians and data scientists. By consistently using this data format, researchers have found they can employ combinations of simple, generalizable tools to perform complex tasks in data processing, analysis, and visualization. 

\begin{itemize}
\item One key concept for improving the reproducibility of experimental data collection is understanding how to create and use structured data formats, of which the ``tidy" data format is one popular implementation. 
\item The ``tidy" data
format plugs into the \textit{tidyverse} framework, which enables powerful and
user-friendly data management, processing, and analysis by combining simple
tools to solve complex, multi-step problems, and this framework is enabled by
ensuring those simple tools share a common interface: a ``tidy" data format
\cite{ross2017declutter, silge2016tidytext, wickham2016ggplot2, wickham2016r}.
\item Working within the R framework facilitates research that adheres to standards of
reproducibility through scriptable data analysis that can easily be placed under
version control \cite{bryan2018excuse}. 
\item Since the tools are simple and share a
common interface, they are easier to learn, use, and combine than tools created
in the classical R framework \cite{ross2017declutter, lowndes2017our,
reviewer2017review, mcnamara2016state}. 
\item This \textit{tidyverse} framework is
quickly becoming the standard taught in introductory R courses and books
\cite{hicks2017guide, baumer2015data, kaplan2017teaching, stander2017enthusing,
reviewer2017review, mcnamara2016state}, ensuring ample training resources for
researchers new to programming, including books (e.g., \cite{baumer2017modern,
lifesciencesR}, some freely available online, e.g., \cite{wickham2016r}),
massive open online courses (MOOCs), onsite university courses
\cite{baumer2015data, kaplan2017teaching, stander2017enthusing}, and Software
Carpentry workshops \cite{wilson2014software, pawlik2017developing}. Further,
tools that extend the tidyverse have been created to enable high-quality data
analysis and visualization in several domains, including text mining
\cite{silge2017text}, microbiome studies \cite{mcmurdie2013phyloseq}, natural
language processing \cite{RJ-2017-035}, network analysis \cite{RJ-2017-023},
ecology \cite{hsieh2016inext}, and genomics \cite{yin2012ggbio}. 
\item The increasingly popular ``tidyverse" framework in the R environment emphasizes the creation and use of many small, general purpose tools that can be easily piped together through consistent interfaces to solve complex analysis and visualization problems. 
\item R's tidyverse has excellent tools that allow you to ``Use many simple models to better understand complex datasets" and to ``turn models into tidy data [with \textit{broom}]. This is a powerful technique for working with large numbers of models because once you have tidy data, you can apply all of the [other tidyverse] techniques. ... The broom package provides a general set of functions to turn models into tidy data." \cite{grolemund2016r} 
\item Specifically, there are many courses, books, etc., that scientists can use to learn general R skills, and many scientists are already skilled in general R. Scientists can leverage general R programming skills to do metabolomics data analysis much more easily if the metabolomics skills follow the same protocals / interface / etc. as more general tools like \textit{ggplot2} and \textit{htmlwidgets}. 
\item ``The grammar of graphics is based on modular components that when combined in different ways will produce different graphics. This enables the user to construct a combinatoric number of plots, including those that were not preconceived by the implementation of the grammar. Most existing tools lack these capabilities." \cite{yin2012ggbio} \item The \textit{ggplot2} framework also offers many other advantages over classic R plotting functions in R for new R users. The \textit{ggplot2} package creates a framework for visualizing data in R using a ``grammar of graphics" approach. This framework is particularly noteworthy for its ability to create multi-layered graphics, and it allows users to add elements to a graphics object to customize elements like color scales and labeling. 
\item ``Creating small reusable components is most in line with the ggplot2 spirit: you can recombine them flexibily to create whatever plot you want." \cite{wickham2016ggplot2} \item The \textit{ggplot2} framework is ``very powerful because you are not limited to a set of pre=specified graphics, but you can create new graphics that are precisely tailored for your problem" \cite{wickham2016ggplot2}. 
\item In particular, it allows a user to learn a series of small, simple tools that can be applied over and over to many different styles of plots, and it separates elements that determine the appearance of the plot (e.g., titles, labeling, colors, themes) from elements that capture the way that the data should be shown (e.g., heatmap, volcano plot). 
\item The \textit{ggplot2} framework includes methods for preventing overplotting, including adding transparency to geoms and binning points to create two-dimensional histograms. \cite{wickham2016ggplot2} 
\item With \textit{ggplot2} and its extensions, an R user will learn basic building block tools for creating graphics and then can use these tools to customize and build any type of plot. 
\item ``Because there is a simple set of core principles and very few special cases, \textit{ggplot2} is also easy to learn. ... Practically, \textit{ggplot2} provides beautiful, hassle-free plots that take care of fiddly details like drawing legends. The plots can be built up iteratively and edited later. A carefully chose set of defaults means that most of the time you can produce a publication-quality graphic in seconds, but if you do have special formatting requirements, a comprehensive theming system makes it easy to do what you want. Instead of spending time making your graph look pretty, you can focus on creating a graph that best reveals the messages in your data." \cite{wickham2016ggplot2} 
\item The \textit{ggplot2} framework includes extensive functionality, so that it R plots are created using this system, many other tools can be used to customize the resulting plots (including customizing color schemes, axis labels, etc.). 
\item Further, these tools are much easier for a new user to learn and extend to many different types of plots than classic R tools. ``Most graphics packages are just a big collection of special cases. For example, in base R, if you design a new graphic, it's composed of raw plot elements like points and lines, and it's hard to design new components that combine with existing plots. 
\item In \textit{ggplot2}, the expressions used to create a new graphic are composed of higher-level elements like representations of the raw data and other statistical transformations, and can be easily combined with new datasets and other plots." \cite{wickham2016ggplot2} 
\item ``One of the key ideas behind \textit{ggplot2} is that it allows you to easily iterate, building up a complex plot a layer at a time. Each layer can come from a different dataset and have a different aesthetic mapping, making it possible to create sophisticated plots that display data from multiple sources." \cite{wickham2016ggplot2}
\item ``In base and lattice graphics, most functions take a large number of arguments that specify both data and non-data appearance, which makes the functions complicated and harder to learn. \textit{ggplot2} takes a different approach: when creating the plot you determine how the data is displayed, then \textit{after} it has been created you can edit every detail of the rendering, using the theming system." \cite{wickham2016ggplot2} The ``pipeline" functionality of \textit{ggplot2} (``+") and \textit{dplyr} (``\%>\%") allow you ``to solve complex problems by combining small pieces that are easily understood in isolation." \cite{wickham2016ggplot2} 
\item The \textit{ggplot2} framework allows R users to create useful, attractive, and easily customized graphics with any dataset that is in a ``tidy" dataframe format. 
\item ``Every layer must have some data associated with it, and that data must be in a tidy data frame. ... A tidy data frame has variables in the columns and observations in the rows." \cite{wickham2016ggplot2} 
\item ``The principle behind tidy data is simple: storing your data in a consistent way makes it easier to work with it. Tidy data is a mapping between the statistical structure of a data frame (variables and observations) and the physical structure (columns and rows). ... Tidy data is particularly important for \textit{ggplot2} because the job of \textit{ggplot2} is to map variables to visual properties: if your data isn't tidy, you'll have a hard time visualizing it." \cite{wickham2016ggplot2} 
\item Under this ``tidy" format, each row of a dataframe gives a single measurement at the level of the unit of observation of interest for the research question, and all variables of interest are included as separate columns. This is particularly appealing because \textit{ggplot2} is now the main plotting framework taught to new R users through popular workshops (e.g., Software Carpentry workshops), massive open online courses [examples], and in many in-person courses [examples / references]. Cheatsheets exist to help building graphics with \textit{ggplot2}. Also, there are a number of other resources for learning this plotting framework, including books (\textit{ggplot2}, \textit{R for Data Science}) and online courses (my Coursera visualization courses, Datacamp course?). 
\item ``In base and lattice graphics, most functions take a large number of arguments that specify both data and non-data appearance, which makes the functions complicated and harder to learn. \textit{ggplot2} takes a different approach: when creating the plot you determine how the data is displayed, then \textit{after} it has been created you can edit every detail of the rendering, using the theming system." \cite{wickham2016ggplot2} 
\item The ``pipeline" functionality of \textit{ggplot2} (``+") and \textit{dplyr} (``\%>\%") allow you ``to solve complex problems by combining small pieces that are easily understood in isolation." \cite{wickham2016ggplot2} 
\item With \textit{ggplot2} and its extensions, an R user will learn basic building block tools for creating graphics and then can use these tools to customize and build any type of plot. 
\item ``Because there is a simple set of core principles and very few special cases, \textit{ggplot2} is also easy to learn. ... Practically, \textit{ggplot2} provides beautiful, hassle-free plots that take care of fiddly details like drawing legends. The plots can be built up iteratively and edited later. A carefully chose set of defaults means that most of the time you can produce a publication-quality graphic in seconds, but if you do have special formatting requirements, a comprehensive theming system makes it easy to do what you want. Instead of spending time making your graph look pretty, you can focus on creating a graph that best reveals the messages in your data." \cite{wickham2016ggplot2} 
\item The \textit{ggplot2} framework includes extensive functionality, so that it R plots are created using this system, many other tools can be used to customize the resulting plots (including customizing color schemes, axis labels, etc.). Further, these tools are much easier for a new user to learn and extend to many different types of plots than classic R tools. 
\item ``Most graphics packages are just a big collection of special cases. For example, in base R, if you design a new graphic, it's composed of raw plot elements like points and lines, and it's hard to design new components that combine with existing plots. In \textit{ggplot2}, the expressions used to create a new graphic are composed of higher-level elements like representations of the raw data and other statistical transformations, and can be easily combined with new datasets and other plots." \cite{wickham2016ggplot2} 
\item ``One of the key ideas behind \textit{ggplot2} is that it allows you to easily iterate, building up a complex plot a layer at a time. Each layer can come from a different dataset and have a different aesthetic mapping, making it possible to create sophisticated plots that display data from multiple sources." \cite{wickham2016ggplot2} 
\item ``In base and lattice graphics, most functions take a large number of arguments that specify both data and non-data appearance, which makes the functions complicated and harder to learn. \textit{ggplot2} takes a different approach: when creating the plot you determine how the data is displayed, then \textit{after} it has been created you can edit every detail of the rendering, using the theming system." \cite{wickham2016ggplot2}
\item The tidyverse ``framework makes it easy for analysts to reshape, combine, group and otherwise manip- ulate data. Packages such as ggplot2, dplyr, and many built-in R modeling and plotting func- tions require the input to be in a tidy form, so keeping the data in this form allows multiple tools to be used in sequence in a seamless analysis pipeline (Wickham, 2009; Wickham and Francois, 2014)." \cite{robinson2014broom}
\end{itemize}

This topic will
be covered in a \textit{Principles} module on ``Principles and power of
structured data formats", two \textit{Implementation} modules on ``The 'tidy'
data format: an implementation of a structured data format" and ``Designing
templates for tidy data collection", and one \textit{Example} module called
``Example: Creating a template for 'tidy' data collection".

\textbf{Managing all research project files in a single, structured directory.}
Reproducbility can also be improved, starting at the data recording stage or
earlier, by using single, structured directory to store all files related to the
project. This ``project" framework has recently been encouraged by a number of
researchers as a way to enable computationally reproducible research, especially
for research conducted by teams \cite{marwick2018packaging,
parker2017opinionated, lowndes2017our}. 

RStudio allows users to create their own
custom ``Project" template, suited to a specific type of data analysis or
software development, which can then be registered and accessed by other users
\cite{rstudioprojecttemplate}. While a ``Project" can have any internal
structure, a common structure can be enforced for a certain type of project
through the creation of a new ``Project" template, which defines the required
subdirectories, structure, and file names of common elements that must exist in
the project \cite{rstudioprojecttemplate}. This template, when selected by a
future user, will create a new directory with a ``skeleton" structure,
potentially including templated files (e.g., for metadata).

\begin{itemize}
\item ``RStudio projects make it straightforward to divide your work into multiple contexts, each with their own working directory, workspace, history, and source documents." \cite{rstudiousingprojects} 
\item RStudio allows users to create ``Projects" for data analysis, which [wonderful things these do], and allow easy integration with version control tools (\textit{git}) and online platforms for sharing a directory under version control (\textit{GitHub}, \textit{GitLab}). In particular, ``Projects" in RStudio can interface directly (with buttons) with GitHub. 
\item ``RStudio v1.1 comes with support for custom, user-defined project templates. Project templates can be used to create new projects with a pre-specified structure. Project templates can be accessed through the New Project's wizard. All registered project templates are made available within the New Directory portion of the wizard. R packages are the primary vehicle through which RStudio project templates are distributed. Package authors can provide a small bit of metadata describing the template functions available in their package. RStudio will discover these project templates on start up, and make them available in the 'New Project...' dialog." \cite{rstudioprojecttemplate}
\item ``Virtually all researchers use computers as a central tool in their workflow. However, our formal education rarely includes any training in how to organise our computer files to make it easy to reproduce results and share our entire analysis pipeline with others. Without clear instructions, many researchers struggle to avoid chaos in their file structures, and so are understandably reluctant to expose their workflow for others to see. This may be one of the reasons that so many requests for details about method, including requests for data and code, are turned down or go unanswered (Collberg and Proebsting 2016)." \cite{marwick2018packaging}
\item ``The goal of a \textbf{research compendium} is to provide a standard and easily recognizable way for organizing the digital materials of a project to enable others to inspect, reproduce, and extend the research. There are three generic principles that define research compendia, independent of particular software tools, and disciplinary contexts. 1. A research compendium should organize its files according to the prevailing conventions of the scholarly community, whether that be an academic discipline or a lab group. Following these conventions will help other people recognize the structure of the project, and also support tool building which takes advantage of the shared structure.
2. A research compendium should maintain a clear separa- tion of data, method, and output, while unambiguously expressing the relationship between those three. In practice, this means data files must be separate from code files. This is important to let others easily identify how the original researcher operated on the data to generate the results. Keeping data and method separate treats the data as 'read-only,' so that the original data are untouched and all modifications are transparently documented in the code. The output files should be considered as disposable, with a mindset that one can always easily regenerate the output using the code and data. The relationship between which code operates on which data in which order to produce which outputs must be specified as well. In his advice to industry data scientists, Ben Baumer's article in this collection simi- larly highlights the importance of keeping data separate from the presentation of data, or research outputs." \cite{marwick2018packaging} 
\item ``In our own experience as researchers, we enjoy benefits of increased efficiency through simplified file management and streamlined analytical workflows. These help us to work more transparently and efficiently. While reproducibility is no guar- antee of correctness, making our results reproducible lets us inspect our own work for errors. Because we follow the same pat- tern in multiple projects, the startup and reentry costs for each project are significantly reduced. The result is that we save time, and minimize the risk of errors that might result from results which cannot be reproduced. A compendium makes it easier to communicate our work with others, including collaborators and (not least of all) our future selves." \cite{marwick2018packaging}
\item When sharing data with statistical collaborators, it is essential to share not just the processed data, but also the raw data and ``an explicit and exact recipe used by the researcher" to go from the raw data to the processed data \cite{ellis2018share}. 
\item With everything from the project collected in a single ``Project" directory, it is much easier to share this collection of material with collaborators. 
\item ``It is critical the researcher include the rawest form of the data. This ensures that data provenance can be maintained through- out the workflow. [This includes] The strange binary file (Binary 2017) that includes all the r measurements taken directly from the machine [and] The hand-entered numbers collected looking through a
microscope ... The researcher knows the raw data are in the right format if
he or she (1) ran no software on the data, (2) did not modify any of the data values, (3) did not remove any data from the dataset, and (4) did not summarize the data in any way. If the researcher has made any modifications to the raw data, it is not the raw form of the data. For example, statisticians are often supplied summary statistics (such as averages) rather than the underlying raw data used to calculate these summary statistics. While the intent of the data collector is to be helpful, the reality is that this slows the analyst down. Statisticians can easily calculate any appropriate summary statistics from the raw data. Being provided the raw data is essential for accurate analysis." \cite{ellis2018share} 
\item ``There should be a section called “Study design” that has a thorough description of the question being asked by the study as well as how the data were collected. An additional section called “Code book” should be provided to describe each variable and its units." \cite{ellis2018share}
\item ``It is important to emphasize that a good workflow starts with good data management and organization practices. The starting place for analyses is (1) primary (“raw”) data, the unpro- cessed data generated from a specific technology such as a sequencing machine, and (2) the data about that data, or metadata (Box 1), that contains information about how the data was collected and generated. Unmodified versions of both raw data and metadata should be stored securely, read-only (so it can’t accidentally be modified) with backups. If any manipulation has been done to the dataset from a facility for quality filtering or other processes, that information should also be retained as part of the metadata. Finally, initial data files should be organized so that they are computer-readable and in nonproprietary formats. These files are the cornerstone of any project and serve as a starting place for reproducing results, starting new analyses or continuing further work." \cite{shade2015computing}
\end{itemize}

This topic will be
covered in a \textit{Principles} module on the ``Power of using a single
structured 'Project' directory for storing and tracking research project files",
an \textit{Implementation} module on ``Creating 'Project' templates", and an
\textit{Example} module called ``Example: Creating a 'Project' template".

\textbf{Implementing version control.} As a research project progresses, a typical practice in many experimental research groups is to save new versions of files (e.g., 'draft1.doc', 'draft2.doc') \cite{bryan2018excuse}, so that changes can be reverted. However, this practice leads to an explosion of files, and it becomes hard to track which files represent the ``current" state of a project. Version control allows researchers to edit and change research project files more cleanly, while maintaining the power to 'backtrack' to previous versions. Further, with version control, messages can be included to explain any changes. For many years, use of version control required use of the command line,
limiting its accessibility to researchers with limited programming experience.
However, graphical interfaces have removed this barrier, and RStudio has 
particularly user-friendly tools for implementing version control. Once a researcher has learned to use git on their own computer for local version control, they can begin using version control 
platforms (e.g., GitLab, GitHub) to collaborate with others in their research
group while keeping the project under version control. These platforms allow
the all collaborators to share a current version of a project directory 
(similar to Dropbox), but in a way that allows easy use of version control 
and that is more efficient for exploring (and, when necessary, undoing) the changes 
each team member has made to project files. 

\begin{itemize}
\item The use of \textit{Git} and \textit{GitHub} has also been encouraged as a tool to enable reproducible research \cite{piccolo2016tools, ram2013git, bryan2018excuse, lowndes2017our, cetinkaya2017infrastructure}. 
\item Projects saved in a single, structured directory
can be easily put under \textit{Git} version control in \textit{RStudio}, which
includes a pane that allows users to work under version control without learning
command-line version control language and, if desired, easily connect the
project with an online version of the project hosted on \textit{GitHub}. 
\item A key source for this section will be \cite{bryan2018excuse}. 
\item ``Many people who do not use Git unwittingly reinvent a poor man’s version of it. Figure 1 depicts a hypothetical analysis of the famous Fisher iris data (FISHER 1936, Anderson (1936)), captured in a single R source file. With informal version control, contributors create derivative copies of iris.R, decorating the file name with initials, dates, and other descriptors. Even when working alone, this leads to multiple versions of iris.R of indeterminate relatedness (Figure 1(a)). In collaborative settings based on email distribution, the original file swiftly becomes part of a complicated phylogeny that no amount of 'Track changes' and good intentions can resolve (Figure 1(b)). The Git way is to track the evolution of iris.R, through a series of commits, each equipped with an explanatory mes- sage. ... Especially important versions get a human- readable tag, for example, “draft-01,” to signal a meaningful milestone. There is some pain in adopting the formalism of Git, but it is worth it." \cite{bryan2018excuse} 
\item ``GitHub complements Git by providing a polished user interface and distribution mecha- nism for Git repositories. Git is the software you will use locally to record changes to a set of files. GitHub is a hosting service that provides a Git-aware home for such projects on the internet. ... GitHub is like DropBox or Google Drive, but more structured, powerful, and programmatic. The remote host acts as the distributor for a Git-managed project. This allows others to browse project files, explore their history, sync up with the current version, and perhaps even pro- pose or make changes. GitHub’s well-designed web interface is a dramatic improvement over traditional Unix Git servers. Many operations can be done entirely in the browser, including editing or adding files. It is easy to create a hyperlink to a specific file or location in a file, at a specific version, which can make meta-conversations about project code or reports much more produc- tive. GitHub also offers granular control over who can see, edit, and administer a project." \cite{bryan2018excuse} 
\item ``GitHub issues are another powerful feature of the platform. Recall that we are repurposing Git, a tool that facilitates software development. Think of the issues for a project as its bug tracker. For projects that are not pure software development, we co-opt this machinery to organize our to-do list more generally. The basic unit is an issue and you can interact with one in two ways. First, issues are integrated into the project’s web interface on GitHub, with a rich set of options for linking to project files and incremental changes. Second, issues and their associated comment threads appear in your email, just like regular messages (this can, of course, be configured). The result is that all correspondence about a project comes through your normal channels, but is also tracked inside the project itself, with excellent navigability and search capabilities." \cite{bryan2018excuse} 
\item ``In http://stat545.com STAT 545 students are required to submit all coursework via GitHub, starting in week one. Most have never seen Git before and do not identify as programmers. It is a major topic in class and office hours for the first 2 weeks. Then we prac- tically never discuss it again." \cite{bryan2018excuse}
\item ``Now the bad news: Git was built neither for the exact usage described here, nor for broad usability. You will undoubtedly notice this, so it is best to know in advance. Happily, there are many helpful tools that mediate your interactions with Git. GitHub itself is a fine example, as is https://www.rstudio.com/products/rstudio/ RStudio (RStudio Integrated Desktop Environment n.d.)." \cite{bryan2018excuse} 
\item ``Git has been repurposed by the data science community (Ram 2013; Bartlett 2016; Perez-Riverol et al. 2016). We use it to manage the motley collection of files that make up typical data analytical projects, which consist of data, figures, reports, and source code. Even those who identify more as statisticians than data scientists generally have a similar mix of files that are the artifacts of a project." \cite{bryan2018excuse} 
\item ``In a Git-based workflow, you document and, optionally, expose your work as you go. Communication and collaboration are the killer apps of version control. Git’s model of file management can feel uncomfortably rigid, but it enables the distribution of files across different people, com- puters, and time." \cite{bryan2018excuse}
\item Version control is defined as: ``automatic management of user edits to any type of document, includ- ing code and workflows. Typically records timestamp, author of the change, and the exact revisions to the file." \cite{shade2015computing}
\item ``All workflows should be maintained using version control so that changes are automatically recorded and authorship to any changes can be attributed easily. Version control is a way of automatically tracking edits to a file, and some non-programming examples include using the “track changes” option in a word processing document, or an automatic backup image of a hard drive. In the literature, there is unwavering support for implementing version control [3,4,12,13]. The sooner biologists become comfortable with the mechanics of version control, the simpler workflow documentation and management will be." \cite{shade2015computing}
\item ``There are several strategies for cultivating a team approach to computational analysis. First, shared storage and workspace, such as on a cloud server or high performance computing clus- ter, can facilitate access to all group data. Raw files from large biological datasets can be main- tained as read-only in a shared storage that can be accessed by all group members. In this way, users will know that the data are both safe and viewable by teammates, which should increase analysis accountability and prevent misconduct in data manipulation. Second, version-con- trolled repository hosting services, such as GitHub, GitLab, or BitBucket can promote team- work by providing easy access to workflows. Research group leaders may create an “organization” for their group and allow team members access to shared repositories where they can sync, share, and track projects easily. GitHub, GitLab, and BitBucket also provide pri- vate repositories that can be shared with editors and reviewers of submitted manuscripts for external reproducibility checkpoints." \cite{shade2015computing}
\item ``version control everything. Version control systems (VCSs) store and back up every previous version of the code, allowing one to ‘roll back’ to an older version of the code when things go wrong. Two of the most popular VCSs are Git [40] (which we recommend) and Subversion [41]. ‘Social coding’ platforms, such as GitHub [42] or Bitbucket [43], are also useful sharing and collaboration tools." \cite{pernet2015improving}
\item ``To increase fMRI reproducibility, neuroscientists need to be trained, and to train others, to plan, document and code in a much more systematic manner than is currently done. Neuroimaging is a computational data science, and most biologists, medical doctors and psychologists lack appropriate programming, software and data science training." \cite{pernet2015improving}
\end{itemize}

This
topic will be covered in two \textit{Principles} modules called ``Harnessing
version control for transparent data recording" and ``Enhance the
reproducibility of collaborative research with version control platforms" as
well as an \textit{Implementation} module on ``Using git and GitLab to implement
version control".

\underline{\textbf{Improving the Reproducibility of Experimental Data
Pre-Processing}}

The second sequence will provide principles and tools for increasing the
computational reproducibility at an early stage of biomedical research, as
experimental data is pre-processed. Examples of pre-processing include ... feature identification and quantification in mass spectrometry data, gating in flow cytometry data, image pre-processing for functional magnetic resonance imaging (fMRI). 

\begin{itemize}
\item ``In the clinical sciences, the definition of which data need to be examined to ensure re- producibility can be contentious. The relevant data could be anywhere along the continuum from the initial raw measurement (such as a pathology slide or image), to the interpreta- tion of those data (the pathologic diagnosis), to the coded data in the computer analytic file. Many judgments and choices are made along this path and in the processes of data cleaning and transformation that can be crit- ical in determining analytical results. Last, even if there is consensus on the appropriate analytical data set, methodologic reproduci- bility requires an understanding of which and how many analyses were performed and how the particular analyses reported in a published paper were chosen. So, whether a particular study is to be considered method- ologically reproducible is contingent on whether there is general agreement about the level of detail needed in the description of the mea- surement process, the degree of processing of the raw data, and the completeness of the analytic reporting." \cite{goodman2016does}
\item 'Raw data' is defined as: ``the un-manipulated data exactly as it is returned by the technology that gen- erated it". \cite{shade2015computing}
\item General preprocessing steps include normalization, scaling, and baseline subtraction (e.g., in mass spec data).
\end{itemize}

This sequence will include nine modules
covering three main topics: 

\textbf{Using code scripts to pre-process experimental data.} The experimental data collected for biomedical research often requires 
pre-processing before it can be analyzed (e.g., gating of flow cytometry data, 
peak finding and quantification for LC / MS metabolomics data). While 
often proprietary point-and-click software is available for this pre-processing,
use of such software can limit the transparency and reproducibility of this 
pre-processing stage of the analysis and is 
time-consuming for repeated tasks over large research projects. 

\begin{itemize}
\item Scriptable
software tools bring key advantages compared to GUI software in terms of data
pre-processing \cite{cetinkaya2017infrastructure, huber2015orchestrating,
preeyanon2014reproducible, piccolo2016tools}, but it is critical to provide some
training on the use of these tools for researchers new to programming. 
\item Expertise
with a scripting language is not universal across the biomedical community,
although literacy in programming is increasing in the sciences
\cite{ram2013git}, and many now recommend programming as a critical skill for
all biology Ph.D. students \cite{list2017ten}. 
\item Open source software ``is likely to be more robust, extensible, modular, and secure than its proprietary competitors" \cite{baumer2017lessons}. 
\item In particular, R is known for its excellent community, so tools created in R will allow new users to tap into this community to ask questions (e.g., through existing R user groups [?] and StackOverflow), and creating of tools in R will also increase the chance that other developers can identify and suggest fixes to any bugs in the software and create their own extensions of the tools we will create \cite{altschul2013anatomy}.
\item When collaborating (e.g., with statisticians), one key element to provide when sharing data for collaboration is ``an explicit and exact recipe used by the researcher" to go from the raw data to a tidy dataset \cite{ellis2018share}. 
\item A script that conducts all the pre-processing of the raw data can provide this ``explicit and exact recipe". 
\item ``Careful statisticians will check the researcher’s recipe, ask questions about steps performed, and try to confirm that they can obtain the same tidy data as the researcher with, at minimum, spot checks." \cite{ellis2018share}
\item ``When it comes to collaborations between data collectors and statisticians, it is a reasonable expectation that the statistician should be able to handle and analyze the data in whatever state they arrive. For this to be possible, the statistician must be pro- vided the raw data, information on any steps taken to prepro- cess the data, and enough information about the experimental conditions to allow the statistician to identify and incorporate hidden sources of variability into his or her analysis (Baggerly 2010)." \cite{ellis2018share} 
\item ``The ideal thing to do when tidying data are to create a com- puter script (in R, Python, or something else) that takes the raw data as input and produces the tidy data being shared as output. Ideally, this script would be run a couple of times to ensure the code produces the same output. Alternatively, in many cases, the person who collected the data may not know how to code in a scripting language. However, he or she still has incentive to make the data tidy for a statistician to speed the process of collaboration. In this case, the statistician should be provided something called pseudocode, which is simply a simple explanation, often broken down into steps, to explain what has been done to the data." \cite{ellis2018share}
\item ``A critical barrier to reproducibility in many
cases is that the computer code is no longer available.
Interactive software systems often used for
exploratory data analysis typically do not keep
track of users’ actions in any concrete form. Even
if researchers use software that is run by written
code, often multiple packages are used, and the
code that combines the different results together
is not saved (10). Addressing this problem will
require either changing the behavior of the software
systems themselves or getting researchers
to use other software systems that are more amenable
to reproducibility. Neither is likely to happen
quickly; old habits die hard, and many will
be unwilling to discard the hours spent learning
existing systems. Non–open source software can
only be changed by their owners, who may not
perceive reproducibility as a high priority." \cite{peng2011reproducible}
\item ``In contrast with data analysts or software developers, many neuroimagers [working with fMRI data] do not code their analysis from scratch - instead they rely on existing software and often reuse code gathered from others in the laboratory or on the web. Pressing buttons in a graphical user interface is not something that can be replicated, unless inputs and processing steps are saved in log files. To ensure reprodu- cibility (even for oneself in a few months’ time) one needs to set up an automatic workflow. Informatics and bioinfor- matics researchers have been discussing issues of code re- producibility for many years [23,24], and lessons can be learnt from their experience. Sandve et al. [24] have a few simple recommendations. First, keep track of every step, from data collection to results, and whenever possible keep track with electronic records. Most neuroimaging software has a so-called batch mode (SPM [25,26]) or pipeline engine (Nipype [27,28]), or is made up of scripts (AFNI [29,30], FSL [31,32]), and saving these is the best way to ensure that one can replicate the analysis. At each step, record electronically, and if possible automatically, what was done with what software (and its version). Sec- ond, minimize, and if possible eliminate, manual editing. For instance, if one needs to convert between file formats, this is better done automatically with a script, and this script should be saved. Third, for analyses that involve a random number generator, save the seed or state of the system, so that the exact same result can be obtained. As for the computer program used to run the experiment (step 1), the batch and scripts can be made available as supplementary material in a journal, and/or shared in re- positories. If one ends up with a fully functional script that includes a new type of analysis, this can itself be registered as a tool on dedicated websites such as the NeuroImaging Tool and Resources Clearinghouse (NITRC [33]). Sharing the analysis batch and scripts is the only way to ensure re- producibility by allowing anyone to (i) check for potential errors that ‘creep in’ to any analyses [10]; (ii) reuse them on new data, possibly changing a few parameters to suit changes in scanning protocol - similar results should be observed if the effects were true [14] - and (iii) base new analysis techniques or further research on verifiable code." \cite{pernet2015improving}
\item ``Bioconductor is an open-source, open-development software project for the analysis and comprehension of high-throughput data in genomics and molecular biology. The project aims to enable interdisciplinary research, collaboration and rapid development of scientific software. Based on the statistical programming language R, Bioconductor comprises 934 [many more now] interoperable packages contributed by a large, diverse community of scientists. Packages cover a range of bioinformatic and statistical applications. They undergo formal initial review and continuous automated testing." \cite{huber2015orchestrating}
\item ``Bioconductor enables the rapid creation of workflows combining multiple data types and tools for statistical inference, regression, network analysis, machine learning and visualization at all stages of a project from data generation to publication." \cite{huber2015orchestrating}
\item The first motivating principle of Bioconductor is: ``The first is to provide a compelling user experience. Bioconductor documentation comes at three levels: workflows that document complete analyses spanning multiple tools; package vignettes that provide a narrative of the intended uses of a particular package, including detailed executable code examples; and function manual pages with precise descriptions of all inputs and outputs together with working examples." \cite{huber2015orchestrating}
\item ``A primary goal is the distributed development of interoperable software components by scientific domain experts. In part we achieve this by urging the use of common data structures that enable workflows integrating multiple data types and disciplines. To facilitate research and innovation, we employ a high-level programming language. This choice yields rapid prototyping, creativity, flexibility and reproducibility in a way that neither point-and-click software nor a general-purpose programming language can. We have embraced R for its scientific and statistical computing capabilities, for its graphics facilities and for the convenience of an interpreted language. R also interfaces with low- level languages including C and C++ for computationally intensive operations, Java for integration with enterprise software and JavaScript for interactive web-based applications and reports." \cite{huber2015orchestrating}
\item ``High-throughput assays such as sequencing, flow cytometry [7] and mass spectrometry continue to decrease in cost and increase in quality. Analyses comprising several assays on the same set of samples are becoming more common. Integrative analysis of multiple data types is perhaps the least standardizable task in genomic data analysis, where the need for a flexible working environment in a high-level language such as R is most apparent.
Integrative analysis of multiple assays generally relies on linking through genomic location or annotation. This includes associating genomic locations with transcripts and protein sequences, proteins with other gene products that function in the same pathway or process, and many other possible associations. The combined computation on multiple linked data types and annotations is the essence of integrative genomic analysis.
To perform such analyses, one must use compatible systems of identifiers, reference genomes, gene models, coordinate systems, and so on. For instance, the identification of RNA-editing sites requires that the user have an accurate genotype for the individual as well as RNA-seq reads aligned to that genotype, and variant calls from a DNA-seq experiment should retain not only information about the alignment software but also the precise version of the genome that was used. Bioconductor software is intended to make it easy and automatic to keep track of such issues. This also helps other analysts to determine whether and how a particular processed data set can be integrated with other data sets." \cite{huber2015orchestrating}
\item ``It can be surprisingly difficult to retrace the computational steps performed in a genomics research project. One of the goals of Bioconductor is to help scientists report their analyses in a way that allows exact recreation by a third party of all computations that transform the input data into the results, including figures, tables and numbers [9]." \cite{huber2015orchestrating}
\item ``Using Bioconductor requires a willingness to modify and eventually compose scripts in a high-level computer language, to make informed choices between different algorithms and software packages, and to learn enough R to do the unavoidable data wrangling and troubleshooting. Alternative and complementary tools exist; in particular, users may be ready to trade some loss of flexibility, automation or functionality for simpler interaction with the software, such as by running single-purpose tools or using a point-and-click interface." \cite{huber2015orchestrating}
\end{itemize}

This topic will be covered in one
\textit{Principles} module on ``Principles and benefits of scripted
pre-processing of experimental data" and two \textit{Implementation} modules
called ``Introduction to scripted data pre-processing in R" and ``Simplify
scripted pre-processing through R's 'tidyverse' tools".

\textbf{Working with complex data types during pre-processing.} Many key R functions output data in a form that is 'untidy' \cite{robinson2014broom}. This is particularly true for raw data from many biomedical experiments, especially machine-generated data (e.g., output from flow cytometer or mass spectrometer). Raw data from many biomedical experiments, especially those that
use high-throughput techniques, can be very large and complex. Because of the 
scale and complexity of these data, software for pre-processing the data in R
often uses complex, 'untidy' data formats. Biocondutor, which hosts many R packages useful for preprocessing and analyzing experimental biomedical data, relies heavily on an object-oriented framework, with functions outputting data in object formats S4 object formats, aiding interoperability among Bioconductor packages and helping to keep together different types of data from an experiment (e.g., for microarray experiments, array-based expression measurements kept together with phenotype and administrative data from the experiment) throughout the code \cite{gentleman2004bioconductor}. These S4 objects are 'untidy', in the sense that they do not follow the format required for 'tidyverse' R tools \cite{biobroom}

While these formats are well-justified within open-source software for pre-processing complex biomedical data, they add a critical barrier for researchers wishing
to implement reproduciblity tools, especially tools from the popular and user-friendly 'tidyverse' collection of R package externsions \cite{robinson2014broom}. While this hurdle can be surmounted by skilled R programmers, it creates a barrier for researchers who are learning scripting tools \cite{robinson2014broom}, and it can reduce transparency of analysis by requiring obscure, lengthy code to extract and tidy data from the complex data object \cite{robinson2014broom}. Very recently, the \textit{broom} and \textit{biobroom} R packages have been developed 
to extract a 'tidy' dataset from many common complex data formats that are output by R functions, including much of the output formats common for preprocessing biomedical experimental data \cite{robinson2014broom, biobroom}.
These tools create a clean, simple connection between the complex data formats
often used in pre-processing experimental data and the 'tidy' format
required to use the 'tidyverse' tools now taught in many introductory R courses, making it more straightforward for a researcher new to R programming develop a scripted R workflow from data preprocessing through to data analysis and visualization using 'tidyverse' tools. The \textit{biobroom} package, in particular, can 'tidy' data within many popular Bioconductor data formats, including \textit{ExpressionSet} objects \cite{biobroom}.

This topic will be covered in a \textit{Principles} module on ``Complex data types in
experimental data pre-processing", a \textit{Implementation} module on ``Complex
data types in R and Bioconductor", and an \textit{Example} module called
``Example: Converting from complex to 'tidy' data formats".

\textbf{Reproducible data pre-processing protocols.} Reproducibility tools can be used to create reproducible data pre-precessing protocols---documents that combine code and text in a ``knitted" document, which can be re-used to ensure data pre-processing is consistent and reproducible across research projects. The R extension package RMarkdown can be used to create documents that combine code and text in a 
``knitted" document, and it has become a popular tool 
for improving the computational reproducibility and 
efficiency of the data analysis stage of research. This tool can also be used earlier in the research process, however, to improve reproducibility of pre-processing steps. 

\begin{itemize}
\item ``For example, in a systematic review of functional magnetic resonance imaging (fMRI) studies, Carp showed that there were almost as many unique analytical pipelines as there were studies26. If several thousand potential analytical pipelines can be applied to high-dimensional data, the generation of false-positive findings is highly likely. For example, applying almost 7,000 analytical pipelines to a single fMRI dataset resulted in over 90\% of brain voxels showing significant acti- vation in at least one analysis27." \cite{munafo2017manifesto}
\item ``Anyone doing any computing in their research
should publish their code. It does not have to be
clean or beautiful (13), it just needs to be available.
Even without the corresponding data, code
can be very informative and can be used to check
for problems as well as quickly translate ideas. Journal
editors and reviewers should demand this so
that it becomes routine. Publishing code is somethingwe
can do now for almost no additional cost.
Free code repositories already exist [for example,
GitHub (http://github.com) and SourceForge (http://
sourceforge.net)], and at a minimum, code can be
published in supporting online material." \cite{peng2011reproducible}
\item ``The roadmap emphasizes tasks that help the user to understand the many choices that inevitably are made as part of a computing workflow. The intent is for the user to systematically query choices in software and parameters, compare options to understand the impact of each on the outcome and interpretation of results, and, ultimately, to make analysis decisions that are most appropriate and biologically or statistically defensible for the dataset in hand. Our roadmap also includes a series of specific internal (e.g., within a research group) and external (e.g., at peer review) checkpoints for repro- ducibility, provides suggestions for effective note-taking in computing, and advocates a team approach to sharing responsibility for analysis and data management." \cite{shade2015computing}
\item Sensitivity analysis is defined as: ``the process of determining which options are most robust to out-
comes in the analysis; sometimes called sensitivity scan". \cite{shade2015computing}
\item Workflow is defined as: ``the complete set of steps required to analyze a dataset from start to finish". \cite{shade2015computing}
\item ``Biologists embarking on the study of a new dataset face many analysis choices. Particularly in bioinformatics, at each step of the process there are multiple software available, each with its own set of variables. Because datasets have unique designs, biases, and underlying assumptions, a default option offered by a software is often not the most appropriate choice and must be re- evaluated for a new dataset. Biologists therefore often decide to try several software packages and analysis variables to determine the best match for their data type and set of questions. This set of all possible combinations of values for the different software and their options can be thought of as parameter space. Here we refer to parameters as the set of all possible options and variables as the options within a given software package. This is analogous to wet bench work, in which there are multiple steps or even protocols in each experiment and decisions must be made as to what components most affect experiment outcomes or are important to vary to optimize results (Table 1). Exploring this parameter space is an important step for a biologist to understand what the tool does and how it performs, and also to interpret the results for the specific dataset in question. However, it is challenging to keep track of the choices and results to effectively evaluate the most appropriate workflow." \cite{shade2015computing}
\item ``In sensitivity analysis, the researcher determines how the outcomes of the analysis are affected by each of the parameter choices (Fig 1). In this approach, one tool and its defaults are selected as a starting point. The user then changes the default value for each variable in turn and determines how each change affects the analysis outcomes. Values can range from defaults to those that are extreme or nonsensical to bracket expectations of potential outcomes. For every change in a variable imposed by the user, the corresponding input and outcome are doc- umented. This process allows the biologist to determine which variables are most important for a given dataset, and while there may not be a universal 'correct' value for each variable, the researcher can evaluate trade-offs and make an informed selection of appropriate values. When the final selection is made for each variable, she can record and annotate them with her rationale for the choices." \cite{shade2015computing}
\item ``From the first moment of an undergraduate research experience, most biologists are trained on the importance of keeping meticulous and up-to-date laboratory notes for posterity, reproduc- ibility of experimental conditions, and attribution of new ideas or techniques. We recommend that biologists approach note-taking for computational projects with the same integrity and accountability. Notes for computing can be embedded directly into most scripting language documents as comments, often designated with a hash (\#). Just as for wet-lab notebooks, grad- uate students and post-doctoral trainees should provide all computing notes to their research team leader when the trainee moves on to her next position. In making recommendations for organizing bioinformatics projects, Noble in 2009 considered each computational “experi- ment” with the same weight as a wet-lab experiment [1], and we champion this perspective and his suggestions therein." \cite{shade2015computing}
\item ``Analysis notes document the consecutive steps taken to explore parameter space for each choice. Like wet-lab experimental notes, the motivation for each step of parameter exploration should be provided, and, afterwards, the test results should be summarized. These notes should also document the rationales for the ultimately selected options. We reiterate the advice of Sandve [3] in taking these analysis notes, especially the need to record all versions of software and to copy lines of executed code verbatim. We also point readers towards Gentzkow et al. [12], who provide excellent advice for workflow documentation (as well as advice regarding other aspects of more advanced workflow, like automation and version control). We recommend maintaining and updating analysis notes in parallel with the draft of the final workflow. There are several options for organizing computational notes, including Jupyter notebooks, Galaxy, Arvados, and knitr for R." \cite{shade2015computing}
\item ``Functional MRI analyses are complex, involving many pre- processing steps as well as a multitude of possible statistical analyses. Even if the most important steps are reported using precise guidelines [21], there are too many parame- ters involved in the data analysis process to be able to pro- vide a full description in any article. Carp [7] examined a simple event-related design using common neuroimaging tools, but varying the available settings (see also [8]). This led to 6,912 unique analysis pipelines, and revealed that some analysis decisions contributed to variability in activa- tion strength, location and extent, and ultimately to inflated false positive rates [4]. In the face of such variability, some have argued that ‘anything less than release of actual source code is an indefensible approach for any scientific results that depend on computation, because not releasing such code raises needless, and needlessly confusing, roadblocks to reproducibility’ [22]." \cite{pernet2015improving}
\item ``Although a simple text editor or word processing document could be used to precisely describe each ana- lysis step, only an executable script and information on the associated software environment can give one a rea- sonable chance of reproducing an entire experiment. This implies that much more should be done to teach programming to students or researchers who need to work with neuroimaging data. Barriers to code sharing are not as great as for data, but they do exist. Re- searchers are often concerned that their code is too poor, and that there might be some errors. These, and the fear of being ‘scooped’, are some of the main reasons scientists give for not sharing code with others [38]. Yet, as Barnes [39] puts it, “software in all trades is written to be good enough for the job intended. So if your code is good enough to do the job, then it is good enough to re- lease”. A few simple rules can be applied to improve scripts [23]. First, make your code understandable to others (and yourself). Add comments to scripts, provid- ing information not just about what is computed, but also reflecting what hypothesis is being tested, or ques- tion answered, by that specific piece of code [24]." \cite{pernet2015improving}
\item ``Neuroimagers can also take advantage of a few easy- to-use tools to create complex scripts and make a work- flow (a workflow consists of a repeatable pattern of activities that transform data and can be depicted as a sequence of operations, declared as work of a person or group (adapted from [44]). ... For Python- based analyses, we recommend the IPython notebook ([49] now the Jupyter project) to sketch the analysis and explore results, along with the workflows provided in Nipype [27,28]. ... Using these pipelines, one can create (via a graphical interface or a script) a workflow of the different steps in- volved in fMRI data processing, specifying parameters needed at each step, and save the workflow. Dedicated libraries or scripts can be called, and the impact of chan- ging a parameter value in a specific implementation of a step can be studied. ... In general, these tools require some programming and software expertise (local installation and configuration issues seem to be largely underestimated issues) beyond what fMRI researchers can usually do (whereas PSOM, Nipype and using the SPM batch system are ‘easy’). These more complex workflow or pipeline solutions can, however, ease replication of the analysis by others: see [53] for an example using the LONI pipeline." \cite{pernet2015improving}
\item ``Experimental protocols in molecular biology are fully pub- lished lists of ingredients and algorithms for creating specific substances or processes. Accuracy of an experimental claim can be checked by complete obedience to the protocol. This standard should be adopted for algorithmic work in [computational biology and bioinformatics]. Portable source code should accompany each published anal- ysis, coupled with the data on which the analysis is based." \cite{gentleman2004bioconductor}
\item ``High-throughput methodologies in CBB are extremely com- plex, and many steps are involved in the conversion of infor- mation from low-level information structures (for example, microarray scan images) to statistical databases of expression measures coupled with design and covariate data. It is not possible to say a priori how sensitive the ultimate analyses are to variations or errors in the many steps in the pipeline. Credible work in this domain requires exposure of the entire process." \cite{gentleman2004bioconductor}
\end{itemize}

This topic will be covered in a \textit{Principles} module called ``Introduction to reproducible data pre-processing protocols", an \textit{Implementation} module on ``RMarkdown
for creating reproducible data pre-processing protocols", and an
\textit{Example} module called ``Example: Creating a reproducible data
pre-processing protocol". 

\underline{\textbf{Choice of tools to include in ``Implementation" modules.}}

\begin{itemize}
\item ``Our collective experience with R suggested that its range of well-implemented statistical and visualization tools would decrease development and distribution time for robust soft- ware for CBB. We also note that R is gaining widespread usage within the CBB community independently of the Bio- conductor Project. Many other bioinformatics projects and researchers have found R to be a good language and toolset with which to work." \cite{gentleman2004bioconductor}
\item ``The R environment includes a well established system for packaging together related software components and docu- mentation. There is a great deal of support in the language for creating, testing, and distributing software in the form of 'packages'. Using a package system lets us develop different software modules and distribute them with clear notions of protocol compliance, test-based validation, version identifi- cation, and package interdependencies. The packaging sys- tem has been adopted by hundreds of developers around the world and lies at the heart of the Comprehensive R Archive Network, where several hundred independent but interoper- able packages addressing a wide range of statistical analysis and visualization objectives may be downloaded as open source." \cite{gentleman2004bioconductor}
\item ``Perhaps the most important aspect of using R is its active user and developer communities. This is not a static language. R is undergoing major changes that focus on the changing techno- logical landscape of scientific computing. Exposing biologists to these innovations and simultaneously exposing those involved in statistical computing to the needs of the CBB com- munity has been very fruitful and we hope beneficial to both communities." \cite{gentleman2004bioconductor}
\item ``A principle that applies across all areas of education is that transference only comes with mas- tery [22]. Courses should therefore stick to one language until learners have progressed far enough with it to be able to distinguish the forest from the trees. While an experienced pro- grammer can, for example, take what they know about loops and function calls in one lan- guage and reuse that understanding in a language with a different syntax or semantics, a newcomer does not yet know which elements of their knowledge are central and which are accidental. Attempting to force transference too early—e.g., requiring them to switch from Python to JavaScript in order to do a web programming course early in their education—will confuse learners and erode their confidence." \cite{brown2018ten}
\end{itemize}

To improve reproducibility practices among our target audience, it is important
that we provide them with some instruction on how to implement the
reproduciblity principles we cover. For some of these principles, there are
several reasonable and well-developed tools that could be used for
implementations. However, few researchers are interested in learning every tool,
and instead would prefer to learn one set of tools that ``just work". 

We have chosen for the implementation portion of these
modules to focus on tools from the open-source R programming language. R can be
freely, quickly, and easily downloaded and installed to a user's computer,
allowing new users to get started quickly, a critical consideration for usable
scientific software \cite{list2017ten}. R has been maintained for over a decade
by the R Development Core Team and works with all major computing platforms,
ensuring  widespread access, stability, and compatability, also critical for
ease-of-use \cite{baumer2017lessons, altschul2013anatomy}. R offers a
well-developed environment for creating new tools that extend the core language
\cite{wickham2015r} and includes ample tools for documenting research workflows
\cite{xie2015dynamic, xie2016bookdown}. R's status as the \textit{lingua franca}
of statisticians and biostatisticians means that its use in early stages of
experimental data recording and pre-processing can help foster closer
collaborations between laboratory-based scientists and statisticians throughout
the research process. R can be scaled as the volume of data in projects grows
\cite{list2017ten}, as it includes tools to interface with distributed computing
platforms (e.g., \textit{Hadoop} \cite{pathak2014rhadoop}, \textit{Spark}
\cite{sparklyr}), and its scripts can be integrated within workflow management
systems (e.g., \textit{Galaxy} \cite{goecks2010galaxy, walker2016models}). Some
of the implementation tools---git and GitLab, for example---are separate from R
but can be mastered much more easily if trainees are taught to use them through
RStudio's user-friendly interfaces rather than using the command line or other
alternative interfaces.

RStudio is a free, open-source Integrated Development Environment (IDE) for the R programming language. It is actively developed by a team of some of the best R programmers worldwide, including the developers of the ``tidyverse" (including the \textit{ggplot2} plotting system), the most popular tools for literate programming in R (\textit{knitr} and \textit{RMarkdown}) and the Shiny web application system. 

We have also selected to focus on this set of tools for
the \textit{Implementation} modules because two members of our team (Anderson
and Lyons) use R daily for their own research and so are able to provide much
better instruction and details on using this set of tools than alternative tools
for the same tasks. Dr. Anderson also regularly teaches students in her
department how to use this set of tools through a graduate-level course and has
developed techniques for helping students new to programming to master R using
the RStudio interface, including using RStudio's interface to use RMarkdown,
track a project under git version control, and connect to version control
platforms to improve collaborative work. 

We appreciate that many researchers do not know the R language, and
some of them may want to learn more about improving reproducibility without
needing to learn a new programming language. For this reason, we have
deliberately separated the ``Principles" and ``Examples" content in our modules
from the ``Implementation" modules, so that a researcher can select a track of
our modules that does not require programming knowledge. Further, while all of
the ``Implementation" modules are conceptually focused on tools that an R
programmer would use, several of the modules could be appreciated and used to
improve reproducibility without a mastery of R. For example, RStudio and its
``Projects" functionality can be used to help manage research project files,
keep them under version control, and interface with GitLab to work
collaboratively without using any R code within the project. Similarly, while
the ``tidy" data format is currently an important implementation common within R
for structuring data, understanding its principals and characteristics does not
require any knowledge of R. In our table of example ``tracks" that a trainee
could follow, the track for an example Principal Investigator gives one example
of a track that would not require any prior knowledge of R or other programming
languages.   

\input{tables/module_content_data_recording.tex}

\input{tables/module_content_data_preprocessing.tex}

\input{tables/module_tracks.tex}

\subsubsection*{Format for the training modules}

\begin{itemize}
\item ``In many cases only bare, unannotated videos of lectures are available, as for the UC Berkeley Webcast resource (http://webcast. berkeley.edu), and these can be highly variable in quality and especially in legibility of projected slides or writing on a blackboard or whiteboard." \cite{searls2012ten}
\item ``In other cases, no video is provided but only detailed syllabi, lecture notes, read- ings, quizzes, exams, and/or demonstra- tions (as for the majority of courses on the MIT OpenCourseWare (OCW) project (http://ocw.mit.edu), and innumerable course web sites). To be sure, these materials can be valuable, and in partic- ular there is a growing trend to posting notes in highly polished form and even as full-fledged online textbooks. But video lectures have many advantages: a sense of immediacy, the feeling of a personal touch, helpful emphasis and nuance in the presentation, and the simple fact that a memorable professor makes for memora- ble subject matter. In such skilled hands, the video format affords the use of techniques that have been shown to enhance learning, including not only visual material but also expressions of enthusiasm by the lecturer and even humor [2]." \cite{searls2012ten}
\item ``Having both lecture videos and ancil- lary course material, as for example with the Open Yale courses to varying degrees (http://oyc.yale.edu), is a huge plus. Even better is the recent trend to structured courseware in which video presentations are modularized and interspersed with assessments and homework. ... All else being equal, the courses offered under this model by initiatives such as Coursera and edX should always be preferred." \cite{searls2012ten}
\item ``Don’t adopt the attitude that the lecture should be a painless way to be spoon-fed the material and that otherwise you may as well just read the textbook on your own. In almost every case the lecturer is doing you the favor of assigning carefully selected readings rather than the entire book, helping you to focus on the ‘‘meat’’ that is most relevant to the approach being taken in the course. Your end of the bargain is to come prepared so that you can most efficiently absorb the value- added of the multimedia presentation." \cite{searls2012ten}
\item ``For courses that are computational in nature, there is the additional imperative to do the programming assignments faith- fully. You haven’t really taken a program- ming course if you haven’t been through the hard slogging: designing, testing, debugging, documenting, refactoring, etc. If the assignment is just too dull, you have the luxury of being able to tweak it towards some variation that interests you, but come what may, it’s well known that you must put in the time to become a proficient coder. In fact, if you are doing general programming classes in an interdisciplin- ary program like bioinformatics, you may want to put in a special effort to modify the programming assignments and projects so that they apply to your domain of interest. You will need experience obtaining and working with the relevant data as much as you need practice with the programming languages and algorithms themselves. It’s hard enough being interdisciplinary with- out doing twice the work, so kill two birds with one stone whenever you can." \cite{searls2012ten}
\item ``Many courses will have quizzes and exams in addition to homework, and once again it is important not to neglect these, for the obvious reason that you need to know how well you are absorbing the material. This is the opportunity to make mid-course corrections, which are far easier to execute in online learning. Moreover, a well-constructed exam can be a learning experience in itself, partic- ularly if it is the occasion for you to whip up your competitive instincts just a bit, since this is likely to improve your retention. Many lines of research point to the benefits of testing in promoting effective learning [6]." \cite{searls2012ten}
\item ``Research from educational psychology suggests that teaching and learning are subject-specific activities [1]: learning programming has a different set of challenges and techniques than learning physics or learning to read and write." \cite{brown2018ten}
\item ``Rather than using slides, instructors should create programs in front of their learners [12]. This is more effective for multiple reasons:
1. It enables instructors to be more responsive to "what if?" questions. While a slide deck is like a highway, live coding allows instructors to go off-road and follow their learners’ interests or answer unanticipated questions.
2. It facilitates unintended knowledge transfer: students learn more than the instructor con- sciously intends to teach by watching how instructors do things. The extra knowledge may be high level (e.g., whether a program is written top-down or bottom-up) or fairly low level (e.g., learning useful editor shortcuts).
3. It slows the instructor down: if the instructor has to type in the program as they go along, they can only go twice as fast as their learners, rather than 10-fold faster as they could with slides—which risks leaving everyone behind. 4. Learners get to see how instructors diagnose and correct mistakes. Novices are going to spend most of their time doing this, but it’s left out of most textbooks.
5. Watching instructors make mistakes shows learners that it’s alright to make mistakes of their own [13]. Most people model the behaviour of their teachers: if the instructor isn’t embarrassed about making and talking about mistakes, learners will be more comfortable doing so too." \cite{brown2018ten}
\item ``Learning to program involves learning the syntax and semantics of a programming language but also involves learning how to construct programs. A good way to guide students through constructing programs is the use of worked examples: step-by-step guides showing how to solve an existing problem." \cite{brown2018ten}
\item ``Learners find authentic tasks more engag- ing than abstracted examples.
A classic question in computing (and mathematics) education is whether problems are bet- ter with context (e.g., find the highest student grade) or without (e.g., find the maximum of a list of numbers). Bouvier et al. [24] examined this with a multiuniversity study and found no difference between the two. They suggest that since it makes no difference, other consider- ations (such as motivation) should be given priority." \cite{brown2018ten}
\item ``Only video courses are included, either showing the instructor with slides and/or blackboard, or in screencast format. Learn- ing from course notes only, or even disembodied audio, simply doesn’t have the immediacy of the visual experience of a lecture hall or even a tablet-based screen- cast. At the other extreme, one could maintain that reading textbooks at one’s own speed is a more efficient and focused way to learn. That is certainly true for some, and perhaps more so for experienced and mature scholars, but it is probably also true that a lecture format offers much- needed structure to the learning process for others. Moreover, cognitive psychology offers both a theoretical basis and empirical evidence for the benefits of multimedia learning [3]. In any case, most of the courses below require reading at least selections from one or more textbooks in close coordination with the lectures (though in a surprising number of cases the textbooks are freely available online)." \cite{searls2012online}
\item ``It is critical to explicitly identify the training objectives and expected outcomes from the outset. Begin by devising the title of your course and specifying the target audience (e.g., laboratory biologists, com- putational scientists). This information is not only useful for trainers to help appropriately focus and weight the con- tents of their training sessions, but is also vital for participants. By explicitly stating the course objectives up front, trainees are better oriented to the expected outcomes and are more likely to be satisfied with the course. As most training sessions are based on slide presentations, dedicate at least one slide (preferably, while providing the session overview) to the learning objec- tives, and mention how these will be achieved, using specific examples whenev- er possible; if appropriate, also mention how the knowledge gained and skill set(s) will be useful for trainees’ work environ- ments. Stating what participants will not learn to do (e.g., to avoid over-estimation of the depth of analysis that can be achieved in a short course) is also impor- tant for tempering their expectations." \cite{via2011ten}
\item ``Verify that trainees’ expectations match what will be delivered. The most effective mechanism to ensure that expectations are well matched is to collect information from trainees prior to the training session itself (e.g., via a questionnaire), or by discussions with trainees at the start of the course. Obtaining such information early on allows time to alter course materials to better meet participant expectations, for example by adjusting case studies and examples to reflect the audience’s interests. Furthermore, this will make you aware of the trainees’ different backgrounds. Read, or listen to, and evaluate all responses, both to discern whether the course content matches participant expectations and to learn what the trainees’ needs are. Such information will also allow you to detect clusters of trainees: e.g., those working with a particular model organism, those more interested in DNA than in proteins, or more plant than animal scientists. Useful information to collect includes their research backgrounds and computational skill sets, their current projects relevant to the course, and their expectations of the training (e.g., what reasons led them to apply for this particular course?). Also solicit information from trainees about the biological problems they wish to solve by participating in the course." \cite{via2011ten}
\item ``Plan the course in independent units/ modules, each with an introduction, set of aims, list of actions, and potential difficul- ties. When a new module is introduced, recall the achievements of the previous module, and state what tasks participants will be able to additionally accomplish at the end of the new module."  \cite{via2011ten}
\item ``When creating your materials and exercises, as much as possible, avoid screen-shots, as these date quickly—oth- erwise, you risk spending substantial amounts of time updating outdated slides rather than concentrating on developing suitable case studies and examples relevant to your audience. Describe the essence of data that can be retrieved from a partic- ular resource and the principles governing a tool, rather than sticking to specific releases, web interfaces, or, for example, to tables of ranked results, which are likely to differ from day to day, as new data become available in the databases." \cite{via2011ten}
\item ``Bioinformatics training encompasses a vast amount of learned skills. Acquiring these skills is a bit like learning to ride a bicycle, where it is best to just start pedalling, because watching others will not help you learn the process! Of course, it is important to provide trainees with the fundamental concepts and theoretical background to ensure that they can use bioinformatics tools and resources mean- ingfully. Nevertheless, it is a good rule to provide a balance between the theoreti- cal/technical and contextual aspects. For example, many trainees may not value information on flat-files, relational sche- mas, APIs, and web services, but will be more concerned about knowing which tools and resources to use for their specific needs, and why, and how to interpret their outputs (just as the average cyclist is not interested in the internal workings of the gearbox, as long as they know how and when to shift gear!). Discuss the limitations of the methods without getting carried away by the intricacies of the algorithms or the minutiae of a tool’s capabilities. Ensure that you cover not only those questions that bioinformatics approaches can answer, but also the limitations of bioinformatics, explicitly illustrating exam- ples that cannot be answered." \cite{via2011ten}
\item ``Wherever possible, provide appropriate biological context: examples without rele- vant context lack meaning and fail to engage trainees. After introducing a new concept, allow time to put the concept immediately into action." \cite{via2011ten}
\item ``Design your materials such that the examples you provide illustrate the con- cepts you wish to convey and, at the same time, are relevant to the research interests of at least some of the trainees. Whenever prior information about trainees’ interests is available, use it. Appreciate that a plant biologist will not have a need for human- centric examples, nor will they find them comparable. The more relevant you make the examples for the trainees, the more likely they are to retain their interest and develop their skills!" \cite{via2011ten}
\item ``Depending on the time available, in- clude quizzes and/or problem-solving tasks and open discussion sessions in which participants can reflect on the skills they’ve learned and how these might be used to address questions of interest to them." \cite{via2011ten}
\end{itemize}

\begin{quotation} ``Describe the \textbf{format} for the training module
proposed and \textbf{justify it in terms of the education goals}."
\end{quotation}

\textbf{Online book.} We will use an online book to collect all the training
materials in a single structure. Each chapter of the book will contain all the
materials from one of the modules listed in Tables \ref{tab:content_one} and
\ref{tab:content_two}, for twenty chapters total. We have created a prototype
(Figure \ref{fig:prototype}) to demonstrate some features of the final book.
Users will be able to quickly navigate through chapters with a navigation bar on
the left of the webpage, with chapter subsection links opening when a chapter is
selected. We will embed the lecture video at the start of the chapter, allowing
a user to watch the video content without leaving the book website, which
prevent having to ``hop around" online to watch the video and then access
written text and additional educational materials in the book. The book will
include a link for the trainee to download a copy as a PDF or EPUB file, to use
as a future reference offline if desired. The format also includes buttons that
can be used to share the link to the online book with others through Twitter and
other platforms, as well as a link to the book's GitHub repository, to allow
early users of the in-development materials to provide feedback on typos, broken
links, unclear materials, and other issues to iron out as we develop the
materials. 

We will create this online book using the \textit{bookdown} framework
\cite{xie2016bookdown}. This in an innovative framework that will allow us to
create a searchable online book that weaves R code into the text and that can
include embedded tutorial videos, active weblinks to online references, and
computationally reproducible practice examples and exercises. Further, by
including R code examples as executable code, we will be able to use this online
book to frequently check tutorial code examples to quickly identify and fix any
broken tutorial code \cite{xie2016bookdown}. We will use GitHub's Git Pages to
freely post this book online. Dr. Anderson (PI) has previously created two
\textit{bookdown}-based books, \textit{R Programming for Research} and
\textit{Mastering Software Development in R}, and has posted and maintained
\textit{R Programming for Research} online continuously since Fall 2016 through
Git Pages.  

\textbf{Video lectures.} Each chapter will include a video lecture that covers
the module's material, with the approximate length of each lecture listed in
Tables \ref{tab:content_one} and \ref{tab:content_two}. We will record these
video lectures in CSU's Computer Assisted Teaching Support laboratory (see
letter from Dr. West), which includes equipment and staff for creating
professional-quality video lectures. We will use YouTube to freely host these
videos and embed the video within the text of that module's chapter in the
online book (see Figure \ref{fig:prototype} for an example of how this will look
to trainees). Trainees will be able to watch each of these videos without
leaving the online books webpage, while hosting them through YouTube will allow
us to take advantage of their excellent, free, and well-tested platform for
sharing videos, as well as allow us to collect detailed analytics on how often
each video is watched and for how long, to help us assess the use of this
component of the training material. 

\textbf{Additional educational materials.} Each module will contain additional
educational material, to help the trainee absorb the material and assess his or
her mastery of the topics. Depending on the module, this additional content will
either be a quiz, questions for discussion, or an applied exercise to try out
the implementation of a tool or principle covered in the module (see Tables
\ref{tab:content_one} and \ref{tab:content_two} for the specific material
planned for each module). For many of the \textit{Implementation} modules, these extra educational materials will be applied examples, since providing tutorials, example code, and example datasets can substantially improve the ability of new users to learn software tools \cite{list2017ten}. To help engage trainees, we will include audio and
video content walking the trainee through answers and solutions for these
additional materials. While not a substitute for in-person training, these video
and audio discussions of the materials is meant to mimic the detailed
walk-throughs and discussions that we would do after a student attempted these
materials if we were teaching these materials in person. We will tape the video
and audio content in CSU's Computer Assisted Teaching Support laboratory (see
letter from Dr. West). We will host the video content through YouTube and the
audio content through SoundCloud and embed this content in the online book, as
with the video lectures. We will use Google Forms as a free and unlimited way
for us to create the quizzes and embed them in the online book [ref].

\noindent \textbf{Insuring compliance with Rehabilitation Act.} We have plans
for making our proposed training module section 508 compliant of the
Rehabilitation Act (29 U.S.C. '794 d), as amended by the Workforce Investment
Act of 1998. Much of the online content will be text based. For figures and
other images in the book, we will use alt and \textit{longdesc} attributes
within the image tag to provide a text alternative. Using YouTube to host the
lecture videos will allow us to draw on their functionality for accessibility,
including ``enough time" requirements in terms of being able to pause and turn
off the content. YouTube allows users to add and edit closed caption content on
their videos, which we will use to add optional closed captions to this
content---in addition to improving accessibility of the content, it may also
help trainees for whom English is a second language to follow the content. In
the first two years of the grant, we will have a student hourly who will assist
Dr. Anderson in the technical implementation of the online book, and helping to
ensure the content is compliant with the Rehabilitation Act will be one of her
key tasks. If this task requires the use of interesting techniques or
technologies, Dr. Anderson and the student may prepare a journal article
describing these techniques and submit it to \textit{The R Journal} during the
project period. The R community is very open to and interested in improving
accessibility, as evidenced by previous publications, presentations, and
software on these topics (e.g., [ref, uswebr ref]).

\subsection{Team}

\input{tables/team_description.tex}

Our team (Table \ref{tab:team_description}) combines experts in R programming (Anderson, Lyons), including its use
to improve the computational reproducibility of health-related research, with
laboratory-based academic researchers in Microbiology and Immunology
(Henao-Tamayo, Gonzalez-Juarrero, Robertson) who are \textbf{attuned to the
needs of and barriers to improving the reproducibility of experimental data
collection and pre-processing among laboratory-based biomedical researchers}.
Our team will allow us to develop training modules that present state-of-the-art
approaches and tools for reproducibility, but do so in a way that is prioritized
to be most useful and accessible to health researchers whose training has
focused on laboratory-related, rather than computational, methods, and for whom
existing training materials on computational reproducibility might be hard to
understand or apply to their own research projects. 

\noindent Our team will also include a to-be-named undergraduate student hourly and Dr. Julie Maertens (Research Associate), a senior evaluator
at the Colorado State University STEM Center, which assists and collaborates
with faculty involved with STEM education-based research and programming in
designing and carrying out evaluations. The student hourly will assist Dr. Anderson
in the technical work of publishing the content our team develops in an online book. 
Dr. Maertens will assist in the design and
implementation of project evaluation throughout this project. Dr. Maertens will
only be involved in the project to assist in planning and implementing
evaluation, and her percent effort is capped at 0.85\% to reflect the RFA's
budget restriction of \$3,000 total on program evaluation, including salary
support.

\subsubsection*{Coordination and management of the team}

\input{tables/team_roles.tex}

The Principal Investigator and all four Co-Investigators will collaborate to 
develop the materials in the training modules. We have designed a plan (Table \ref{tab:roles}) in which, for each module, Dr. Anderson and one of the Co-Investigators will collaborate as the primary authors of the written text, lecture slides, and additional education materials (quiz, discussion questions, or applied exercise). A second co-investigator will serve as the first tester of the module and will work through the module material and provided detailed feedback to the two authors of the module for refining the material. The training materials for the module will then be tested on the next CSU pilot testing session (sessions will occur twice per year over the project period), and further refined based on that feedback, feedback from the two Co-Investigators who did not have the roles of author or first tester for that module, and any feedback from early online users. Dr. Anderson will then film the video lecture for the module. Throughout the project period, Dr. Anderson and the four Co-Investigators will meet as a group bi-monthly to check in on progress on the project. 

Throughout the module development period (first two years of the project), Dr. Anderson and the undergraduate student will transfer the developed training content into the online book format and publish it online. Dr. Maertens will assist Dr. Anderson at points throughout the project period to develop materials for evaluation, including feedback surveys to use with CSU pilot testers, survey questions to include in the online book for evaluation purposes, and brief training on how to best elicit qualitative feedback from pilot testers during the biannual CSU pilot testing sessions.

Our plan of content development and publishing is ambitious, but we are
confident we can meet it. Dr. Anderson previously developed the online content,
in collaboration with a co-instructor, for a five-course specialization with
approximately twice as much content in under nine months. She has experience
developing and publishing online training materials for learning R-based tools,
including through the \textit{bookdown} online book framework we plan to use
here. In this previous development of online training materials, Dr. Anderson
learned to collaborate closely with co-authors in developing and refining the
content and in developing quizzes and applied exercises to allow trainees to
test their understanding of the content. She will bring that experience in the
proposed project, to help organize and integrate the contributions of our team
of co-investigators. For the technical development of the book, she will be
assisted by and supervise an undergraduate hourly during the first two years of
the project, to help in turning the intellectual content that she and the
co-investigators develop into the formatted online book. She will work with Dr.
Maertens to solidify and implement evaluations of the training materials.

\subsection{Institutional Environment and Commitment}

Colorado State University is a Research-I institution, with vibrant research programs in a variety of scientific, engineering, and health-related fields. As Colorado's land-grant university, CSU has a 100-year-old extension program to help researchers disseminate their research results to members of the wider community. Colorado State University is very supportive of improving the reproducbility of
research, and it offers ample resources that will help us ensure the success of
the proposed project. \textbf{Our institution has clearly expressed its support
for our proposed effort to create, refine, evaluate, and disseminate these
training materials} in institutional letters of support from Dr. Jac Nickoloff
(Chair of the Department of Environmental \& Radiological Health Sciences, CSU),
Dr. Dean Gregg (Chair of the Department of Microbiology, Immunology, \&
Pathology, CSU), and Dr. Alan Rudolph (Vice President for Research, CSU). As
expressed in the letter of support from Dr. Nickoloff, Dr. Anderson's teaching
expectations during the project period are capped at three courses every two
years, allowing adequate time for her to complete the tasks required by this
proposal. As also expressed in that letter, Dr. Anderson's department supports
that the training materials developed under this grant will be made freely
available through online publication under a Creative Commons license. Dr.
Anderson's department has supported her previous development and publication of
similarly free and open training materials following a similar format [ref].

Colorado State University is host to a large number of laboratory-based
biomedical researchers, especially in fields related to drug and vaccine
development for infectious diseases. We will \textbf{take advantage of this
environment} to help us pilot test and refine the training materials, to ensure
they are clear, relevant, and useful to our target population of
laboratory-based biomedical researchers. We have received statements of support
from relevant programs, including the the director of the CSU Interdisciplinary
Graduate Program in Cell \& Molecular Biology, and Associate Director of the
Interdisciplinary NSF-NRT GAUSSI training program in Computational Biology, to
help disseminate our materials to pilot testers and early users of our training
materials within CSU (see letters from Drs. Dean Gregg, Carol Wilusz, and Jeff
Wilusz). 

Colorado State University offers many excellent facilities that we will take
advantage of to help us achieve our project's goals. CSU provides meeting rooms
that we can reserve free-of-charge to use for the CSU-based user testings. The
Computer Assisted Teaching Support (CATS) laboratory at CSU's Academy for
Teaching and Learning has professional-grade recording equipment that we will
use to record the video lectures within each module (see letter from Dr. Andrew
West). CSU's STEM Center will provide help in evaluating the training modules,
including through the budgeted involvement of Dr. Julie Maertens, a senior
evaluator at the center. 

\subsection{Piloting and evaluating the effectiveness of the training modules}

We will conduct two, day-long user testing sessions at CSU in each year of the
project. The CSU user testing groups will consist of current and future
laboratory-based biomedical researchers. We will recruit trainees with a variety
of research roles, including undergraduate students, graduate students,
postdoctoral fellows, research associates, and principal investigators. We have
informed several CSU biomedical researchers about these proposed testing
sessions and received their support in encouraging CSU researchers within their
groups and departments to participate (see letters from ...). 

Table \ref{tab:evaluation}.

In the first two project years, each session will test the set of modules
developed since the last user testing (approximately five modules will be tested
each of these session). The sessions will begin with our team giving live
lectures of the same content we plan to film for the video lectures included in
the online book. This will allow us to improve and refine this content, based on
detailed feedback from testers representative of our target audience, before
filming the final video lectures. During the rest of the session, we will divide
the trainees into small teams to work through the additional educational
materials (applied exercises, quiz questions, and discussion questions) to iron
out problems with the clarity or implementation of these materials. The trainees
will have access to the in-development online book as they work through these
materials. In the last year of the project, these sessions will revisit the material
in modules that proved problematic in their first round of pilot testing, allowing us
to re-test approximately ten of the modules (five tested per session in project year 3).

[Information we will collect from these training sessions.]

Dr. Anderson (PI) has experience in productively conducting these kinds of user
testing sessions at CSU. She has run several two-hour user testing sessions with
students from various departments of Colorado State University prior to
releasing R software packages \cite{futureheatwaves, countyweather}. Further, in
April 2016, she led a longer, two-day user testing session through a Weather
Data Hackathon at Colorado State University (Figure \ref{csu-r-hackathon}).
Around 15 people participated, including undergraduate students, graduate
students, postdoctoral fellows, and professors from CSU's Departments of
Atmospheric Sciences, Civil \& Environmental Engineering, Microbiology, and
Statistics. Some of the ideas and code developed during this Hackathon have
since led to development and publication of open source software
\cite{countyfloods, noaastormevents}.

\begin{SCfigure} \centering \includegraphics[width =
0.6\textwidth]{figures/csu_hackathon.png} \caption{Some of the approximately 15
undergraduate students, graduate students, postdoctoral fellows, and professors
who participated in a two-day Weather Data Hackathon at Colorado State
University in April 2016 led by Dr. Anderson.} \label{csu-r-hackathon}
\end{SCfigure}

\begin{quotation} ``Applications must include a plan for evaluating the
activities supported by the award in terms of their \textbf{frequency of use}
and their \textbf{usefulness}. The use of \textbf{multiple evaluation
approaches} is highly encouraged as is \textbf{testing several groups with
different characteristics}. The application must specify \textbf{baseline
metrics (e.g., numbers, educational levels, and demographic characteristics of
test group)} in a structured format, as well as \textbf{measures to gauge the
short and long-term success of the research education award in achieving its
objectives}. Applicants are expected to \textbf{obtain feedback from test group}
to help identify weaknesses and to provide suggestions for improvements, and
\textbf{make the evaluation and feedback data} available to NIGMS staff."
\end{quotation}

For each of the modules, we have outlined \textbf{learning objectives} will help us evaluate if the training modules are achieving their educational goals.

\textbf{Evaluation methods}:

\begin{itemize}
\item Google Analytics for online book's website.
\item YouTube analytics for the embedded videos. 
\item Quiz for some modules. Use to evaluate how well they've mastered the material. Use Google Forms to embed these quizzes.
\item Link to voluntary survey in each chapter of the online book: Educational level, demographic characteristics. Includes rating for the usefulness of that module.
\end{itemize}

\input{tables/evaluation.tex}

\textbf{Groups for piloting and evaluation:}

\begin{itemize}
\item \textbf{Final users of the online book and videos.} These could potentially be from anywhere in the world, and for many we won't have great ways to contact them. 
\item \textbf{Workshop attendees for the workshops we plan to propose and do at national microbiology / immunology conferences.} For these people, we could definitely do a survey before to get information on demographics, education level, interest in the training materials, etc. We could also do a post survey to find out what they learned, how helpful it was, what they found confusing, etc. Finally, we could get their email addresses to ask some longer-term evaluation questions (e.g., How are they using what they learned 1--2 years after taking the workshop? How much did they retain in terms of principals, implementation, and examples?). We can also use questions that are asked during the workshops and areas where additional materials (applied exercises, quizzes, discussion questions) are problematic to help us hone our training materials.
\item \textbf{Early online users.} We will plan to develop and post the text and some of the additional educational materials (e.g., quizzes, discussion questions) online through GitHub \textit{as we write the book and develop the materials}. We will use social media to invite people to try out the book as we develop it. Based on previous work developing online books, we have found that this open development process can help attract users very early in the process, and that these users are often very helpful in providing feedback as the book is developed. We will elicit their feedback through GitHub (``Issues" page will be the main forum for them to post comments and suggestions).
\item \textbf{CSU pilot users.} We can ask these pilot users many questions, both before and after the pilot testing. Further, we will have access to ask them longer-term outcomes, as well as to ask at the department level how the use of these training materials by a number of people in the department has changed research practices and what is considered ``best practice" for research in the department (i.e., a `bubble up" effect).
\item \textbf{Pilot users from other institutions.} Similar to CSU pilot users, although we'll have a bit less access for determining longer-term and department-wide outcomes.
\end{itemize}
    
Examples of what we hope to learn from each group in pilot testing and evaluation (Tables \ref{tab:evaluation}).

\subsection{Dissemination Plan}

We will use GitHub's free ``Pages" website hosting to publish the book freely online, with all materials published under the Creative Commons Attribution-ShareAlike 4.0 International License [ref], \textbf{making all materials freely accessible, both nationally and internationally}. From the beginning of the project, we will publish the book online as it develops, and we will promote this material through postings on social media (e.g., Twitter) and take advantage of our network of colleagues in biomedical research and the R programming community to help promote the availability of the materials (for example, see letter of support from Dr. Peng). This book will be hosted on a ``Git Pages" webpage, allowing the online book to be easily accessed through a link posted on the NIGMS's \textit{Clearinghouse for Training Modules to Enhance Data Reproducibility}. There will be no paywall or other restriction on accessing any of the training materials, and source code for the book and exercises will be published on GitHub. All video and audio content will be published online in free formats through YouTube and SoundCloud, with the content embedded in the online books webpage. At the end of project years 2 and 3, we will also post a static version of the book to the website \textit{bookdown.org}, where people can go to find free online books published using the bookdown format and which invites direct submissions from authors that have used these framework. The PI has previously had substantial success in disseminating online training materials. She is the co-instructor of a five-course specialization on \textit{Mastering Software Development in R} through the Massive Open Online Course platform Coursera. This series has had over 50,000 participants since it was opened in Fall 2016, and an accompanying online book on the LeanPub platform has been downloaded by over 14,000 people.

In addition to these methods of disseminating the training materials to a general audience, \textbf{we will also take specific steps to make sure that our target audience---laboratory-based biomedical scientists---are aware of these training materials.} We will apply to present posters or oral presentations in years 2 and 3 of the project at three national and international conferences (American Society for Microbiology Conference, American Association of Immunologists Meeting, and International Society for the Advancement of Cytometry) to pilot the training materials and help get out the news among our target audience that these materials are freely available. We will also apply to present a workshop in year 2 of the project at the American Society for Microbiology Conference to pilot the training materials and help get out the news among our target audience that these materials are freely available. We will pilot our training materials among CSU researchers in our target audience through biannual, day-long user testing; in addition to providing us with feedback to help refine our materials, this will help us let members of our target audience know that these training materials are freely available and how to find them. We will also invite colleagues (and their research group members) at outside of CSU to serve as early pilot testers of the online version of all our materials. In addition to providing us with feedback to help refine our materials, this will help us let members of our target audience know that these training materials are freely available and how to find them. Finally, we will write and submit a paper describing these training materials and highlighting their content in a biomedical journal relevant to our target audience, like [example of a couple of journals].

\subsection{Timeline}

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{figures/timeline.pdf}
    \caption{Timeline for proposed activities for this project.}
    \label{fig:timeline}
\end{figure}

\clearpage

\bibliographystyle{unsrtnat}
\bibliography{rep_modules}

\end{document}
